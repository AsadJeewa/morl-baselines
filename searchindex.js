Search.setIndex({"docnames": ["algos/algorithms", "algos/multi_policy", "algos/multi_policy/envelope", "algos/multi_policy/mp_mo_q_learning", "algos/multi_policy/ols", "algos/multi_policy/pareto_q_learning", "algos/multi_policy/pgmorl", "algos/single_policy", "algos/single_policy/eupg", "algos/single_policy/moq_learning", "community/community", "features/buffers", "features/misc", "features/networks", "features/pareto", "features/performance_indicators", "features/scalarization", "index", "quickstart/overview"], "filenames": ["algos/algorithms.md", "algos/multi_policy.md", "algos/multi_policy/envelope.md", "algos/multi_policy/mp_mo_q_learning.md", "algos/multi_policy/ols.md", "algos/multi_policy/pareto_q_learning.md", "algos/multi_policy/pgmorl.md", "algos/single_policy.md", "algos/single_policy/eupg.md", "algos/single_policy/moq_learning.md", "community/community.md", "features/buffers.md", "features/misc.md", "features/networks.md", "features/pareto.md", "features/performance_indicators.md", "features/scalarization.md", "index.md", "quickstart/overview.md"], "titles": ["Overview", "Multi-Policy Algorithms", "Envelope Q-Learning", "MPMOQ Learning", "OLS", "Pareto Q-Learning", "PGMORL", "Single-policy Algorithms", "EUPG", "MOQ-Learning", "Community", "Replay Buffers", "Miscellaneous", "Neural Networks helpers", "Pareto utils", "Performance indicators", "Scalarization functions", "MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.", "Overview"], "terms": {"morl": [0, 2, 3, 5, 6, 8, 9, 10, 12, 18], "baselin": [0, 2, 3, 5, 6, 8, 9, 10, 12, 18], "contain": [0, 11, 12, 17, 18], "multipl": [0, 2, 11], "implement": [0, 10, 11, 17, 18], "multi": [0, 2, 3, 5, 6, 8, 9, 13, 15, 18], "object": [0, 2, 3, 4, 5, 6, 8, 9, 15], "reinforc": [0, 2, 3, 5, 6, 8, 9, 11, 13], "learn": [0, 6, 8, 10, 11, 13], "algorithm": [0, 2, 3, 6, 8, 9, 10, 11, 15, 18], "The": [0, 2, 5, 8, 11, 12, 13, 14, 18], "follow": [0, 11, 12, 13, 17, 18], "tabl": [0, 9], "list": [0, 2, 4, 6, 8, 11, 13, 15], "ar": [0, 4, 10, 11, 15, 17], "current": [0, 2, 4, 5, 10, 11, 12, 13, 15], "algo": 0, "singl": [0, 9, 17, 18], "polici": [0, 2, 3, 5, 8, 9, 17, 18], "esr": [0, 8, 11, 17, 18], "ser": [0, 9, 17, 18], "observ": [0, 2, 8, 11, 13], "space": 0, "action": [0, 2, 5, 8, 9, 11], "paper": [0, 2, 3, 4, 5, 6, 8, 9, 13, 17], "envelop": 0, "q": [0, 3, 9, 10], "continu": [0, 6], "discret": 0, "pgmorl": [0, 15], "supplementari": [0, 6], "materi": [0, 6], "pareto": [0, 10, 15, 17], "mo": [0, 3, 6, 9, 17, 18], "mpmoqlearn": [0, 3], "outer": [0, 3, 4], "loop": [0, 3, 4], "moql": 0, "optimist": [0, 4], "linear": [0, 4], "support": [0, 4], "ol": 0, "section": [0, 4], "3": [0, 4], "thesi": [0, 4], "expect": [0, 8], "util": [0, 8, 9, 12, 13, 17], "gradient": [0, 8], "eupg": [0, 10], "class": [2, 3, 4, 5, 6, 8, 9, 11, 13, 14], "morl_baselin": [2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17], "multi_polici": [2, 3, 4, 5, 6, 18], "env": [2, 3, 5, 6, 8, 9], "learning_r": [2, 3, 6, 8, 9], "float": [2, 3, 4, 5, 6, 8, 9, 11, 12, 15, 16], "0": [2, 3, 4, 5, 6, 8, 9, 11, 12], "0003": [2, 6], "initial_epsilon": [2, 3, 5, 9], "01": [2, 11, 12], "final_epsilon": [2, 3, 5, 9], "epsilon_decay_step": [2, 3, 9], "option": [2, 3, 4, 5, 6, 8, 9, 12], "int": [2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16], "none": [2, 3, 4, 5, 6, 8, 9, 11, 12], "tau": [2, 12, 16], "1": [2, 3, 4, 5, 6, 9, 11, 12], "target_net_update_freq": 2, "1000": [2, 3, 8, 9], "buffer_s": [2, 8], "1000000": 2, "net_arch": [2, 6, 8, 13], "256": 2, "batch_siz": [2, 11], "learning_start": [2, 3, 9], "100": [2, 5, 6], "gradient_upd": 2, "gamma": [2, 3, 5, 6, 8, 9], "99": [2, 5, 8], "max_grad_norm": [2, 6], "bool": [2, 3, 4, 5, 6, 8, 9, 11, 15], "true": [2, 3, 4, 5, 6, 8, 9, 11], "num_sample_w": 2, "4": [2, 6], "per": [2, 5, 9, 13], "per_alpha": 2, "6": [2, 6], "initial_homotopy_lambda": 2, "final_homotopy_lambda": 2, "homotopy_decay_step": 2, "project_nam": [2, 3, 5, 6, 8, 9], "str": [2, 3, 5, 6, 8, 9, 12, 15], "experiment_nam": [2, 3, 5, 6, 8, 9], "log": [2, 3, 5, 6, 8, 9, 12], "devic": [2, 6, 8, 11], "union": [2, 6, 8, 15], "auto": [2, 6, 8], "lean": 2, "us": [2, 5, 8, 9, 11, 12, 13, 18], "condit": [2, 8], "network": [2, 8, 12], "emb": 2, "take": 2, "weight": [2, 4, 8, 9, 12, 16, 17], "input": [2, 12, 13, 14], "main": [2, 10, 11], "chang": [2, 14], "thi": [2, 10, 11, 12, 13, 14, 16, 17, 18], "compar": 2, "scalar": [2, 3, 4, 8, 9, 12], "cn": 2, "dqn": [2, 12, 13], "i": [2, 4, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18], "target": [2, 11, 12], "updat": [2, 8, 9, 11, 12], "r": 2, "yang": 2, "x": [2, 12], "sun": 2, "k": [2, 3, 5, 9], "narasimhan": 2, "A": [2, 3, 5, 8, 9, 11, 12], "gener": [2, 3, 6, 8, 9, 12], "adapt": [2, 16], "arxiv": 2, "1908": 2, "08342": 2, "c": 2, "nov": [2, 6], "2019": 2, "access": 2, "sep": 2, "06": 2, "2021": 2, "onlin": 2, "avail": [2, 6, 11, 17], "http": [2, 4, 6, 11, 14, 17], "org": 2, "ab": 2, "act": 2, "ob": [2, 4, 8, 9, 11], "tensor": [2, 11, 12, 13], "w": [2, 4, 6, 8, 9], "epsilon": [2, 4, 12], "greedili": [2, 9], "select": [2, 4, 5], "an": [2, 4, 5, 10, 13], "given": [2, 5, 8, 11, 12], "paramet": [2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17], "vector": [2, 4, 5, 12, 13, 14, 15, 16], "return": [2, 3, 4, 5, 6, 8, 9, 11, 12, 14, 15, 16, 17], "integ": 2, "repres": [2, 13], "ddqn_target": 2, "doubl": 2, "envelope_target": 2, "sampled_w": 2, "comput": [2, 5, 11, 12, 15], "set": [2, 4, 5, 11, 14, 15, 18], "sampl": [2, 11], "eval": [2, 8, 9], "ndarrai": [2, 3, 4, 5, 6, 8, 9, 12, 13, 14, 15, 16], "give": [2, 8, 11], "best": [2, 8, 9, 16], "np": [2, 4, 8, 15], "arrai": [2, 6, 8, 9, 11, 17], "get_config": [2, 3, 5, 6, 8, 9], "dictionari": [2, 3, 5, 6, 8, 9, 11, 12], "configur": [2, 3, 5, 6, 8, 9], "dict": [2, 3, 5, 6, 8, 9, 12], "config": [2, 3, 6, 8, 9], "load": 2, "path": 2, "load_replay_buff": 2, "model": 2, "replai": [2, 18], "buffer": [2, 8, 17, 18], "specifi": [2, 11], "whether": [2, 9, 11], "too": 2, "max_act": 2, "highest": [2, 4], "valu": [2, 4, 5, 11, 12, 15, 16], "save": 2, "save_replay_buff": 2, "save_dir": 2, "filenam": 2, "directori": 2, "train": [2, 3, 5, 6, 8, 9, 12], "total_timestep": [2, 8, 9], "total_episod": 2, "reset_num_timestep": [2, 9], "eval_env": [2, 8, 9], "eval_freq": [2, 3, 8, 9], "reset_learning_start": 2, "fals": [2, 4, 6, 11], "agent": [2, 3, 6, 8, 9, 12], "total": 2, "number": [2, 5, 8, 9, 11, 12, 13], "timestep": [2, 8, 9, 12], "If": [2, 4, 10, 11], "randomli": 2, "everi": [2, 5], "episod": [2, 5, 8, 12], "done": [2, 11], "ignor": 2, "reset": [2, 9], "when": [2, 9, 11], "time": [2, 9], "environ": [2, 8, 9, 17, 18], "evalu": [2, 3, 5, 8, 9, 14], "frequenc": [2, 8], "start": [2, 9, 11], "": [2, 6, 8, 11, 12, 17, 18], "e": [2, 8, 11, 17], "g": [2, 8, 17], "experi": [2, 8, 11, 17], "from": [2, 4, 5, 8, 10, 11, 12, 13, 15, 17], "multi_policy_moqlearn": 3, "mp_mo_q_learn": 3, "ref_point": [3, 5, 6, 15], "numpi": [3, 9, 11, 16, 17], "weights_step_s": 3, "function": [3, 5, 9, 11, 12, 13, 14, 17], "weighted_sum": [3, 9, 16], "9": [3, 9], "type": [3, 9, 13], "num_timestep": 3, "500000": [3, 9], "multipolici": 3, "moq": 3, "version": 3, "mo_q_learn": [3, 9], "van": [3, 5, 9], "moffaert": [3, 5, 9], "m": [3, 9], "drugan": [3, 9], "now": [3, 8, 9], "novel": [3, 9], "design": [3, 9], "techniqu": [3, 9], "2013": [3, 9], "doi": [3, 9], "10": [3, 6, 9], "1109": [3, 9], "adprl": [3, 9], "6615007": [3, 9], "eval_all_ag": 3, "all": [3, 4, 10, 11, 14, 17, 18], "reward": [3, 8, 16, 17], "discount": 3, "tupl": [3, 11], "num_object": 4, "verbos": 4, "method": [4, 5, 9, 11, 12], "next": [4, 11], "roijer": [4, 8], "info": [4, 12], "pub": 4, "pdf": [4, 6], "add_solut": 4, "add": [4, 11, 14], "new": [4, 10, 11], "optim": [4, 5, 17], "indic": [4, 11], "remov": [4, 11, 14], "cc": 4, "being": 4, "domin": [4, 5, 14, 16], "compute_corner_weight": 4, "corner": 4, "see": [4, 18], "definit": [4, 17], "19": 4, "typo": 4, "sign": 4, "should": [4, 11], "end": [4, 11], "queue": 4, "empti": 4, "extrema_weight": 4, "which": [4, 11, 12, 13, 17], "have": [4, 10, 12, 14, 17], "one": [4, 9], "compon": [4, 16], "equal": 4, "rest": 4, "get_ccs_weight": 4, "get_corner_weight": 4, "top_k": 4, "get_prior": 4, "get": [4, 5, 10, 11], "prioriti": [4, 11, 12], "is_domin": 4, "check": [4, 11], "ani": 4, "otherwis": 4, "max_scalarized_valu": 4, "maximum": [4, 11], "max_value_lp": 4, "w_new": 4, "upper": 4, "bound": 4, "next_weight": 4, "remove_obsolete_valu": 4, "visit": 4, "remove_obsolete_weight": 4, "new_valu": 4, "better": 4, "than": 4, "previou": [4, 11], "pareto_q_learn": 5, "pql": 5, "8": 5, "epsilon_decai": 5, "seed": [5, 6, 12], "tabular": 5, "reli": [5, 9, 15, 16], "prune": [5, 17], "now\u00e9": 5, "journal": [5, 17], "machin": [5, 6], "research": 5, "vol": 5, "15": 5, "pp": [5, 6], "3483": 5, "3512": 5, "2014": 5, "calc_non_domin": 5, "state": 5, "non": [5, 14], "get_local_pc": 5, "collect": [5, 13], "local": 5, "pc": 5, "default": [5, 11], "get_q_set": 5, "pair": [5, 11], "score_hypervolum": 5, "score": 5, "base": [5, 8], "upon": 5, "hypervolum": [5, 15], "metric": [5, 15], "score_pareto_cardin": 5, "cardin": 5, "select_act": 5, "score_func": 5, "callabl": [5, 16], "track_polici": 5, "vec": 5, "track": 5, "its": [5, 11], "array_lik": 5, "num_episod": 5, "3000": 5, "log_everi": 5, "action_ev": 5, "front": [5, 14, 15], "result": 5, "name": 5, "final": [5, 12], "env_id": 6, "halfcheetah": 6, "v4": 6, "5": [6, 9], "num_env": 6, "pop_siz": 6, "warmup_iter": 6, "80": 6, "steps_per_iter": 6, "2048": 6, "limit_env_step": 6, "5000000": 6, "evolutionary_iter": 6, "20": 6, "num_weight_candid": 6, "7": 6, "num_performance_buff": 6, "performance_buffer_s": 6, "2": [6, 11], "min_weight": 6, "max_weight": 6, "delta_weight": 6, "995": 6, "torch_determinist": 6, "64": 6, "num_minibatch": 6, "32": 6, "update_epoch": 6, "anneal_lr": 6, "clip_coef": 6, "ent_coef": 6, "vf_coef": 6, "clip_vloss": 6, "norm_adv": 6, "target_kl": 6, "gae": 6, "gae_lambda": 6, "95": 6, "predict": [6, 13], "guid": [6, 17], "refer": [6, 15, 16], "j": 6, "xu": 6, "y": 6, "tian": 6, "p": 6, "ma": 6, "d": [6, 8], "ru": 6, "sueda": 6, "matusik": 6, "robot": 6, "control": [6, 13], "proceed": 6, "37th": 6, "intern": 6, "confer": 6, "2020": 6, "10607": 6, "10616": 6, "mlr": 6, "press": 6, "v119": 6, "xu20h": 6, "html": 6, "peopl": [6, 10], "csail": 6, "mit": 6, "edu": 6, "jiex": 6, "supp": 6, "single_polici": [8, 9, 18], "100000": [8, 11], "50": 8, "001": 8, "idea": [8, 10], "accru": 8, "futur": 8, "steckelmach": [8, 10], "2018": 8, "accrued_reward": [8, 11], "run": 8, "moqlearn": 9, "id": [9, 11, 12], "parent_writ": 9, "torch": [9, 12, 13], "tensorboard": 9, "writer": [9, 12], "summarywrit": [9, 12], "maintain": 9, "choos": 9, "move": [9, 11], "start_tim": 9, "max": 9, "recal": 9, "other": [9, 11, 17], "launch": 9, "greedi": 9, "between": [9, 15], "each": [9, 11, 13, 15, 16], "we": [10, 11, 15, 17], "discord": 10, "server": 10, "where": 10, "you": 10, "can": 10, "ask": 10, "question": [10, 14], "help": 10, "repositori": [10, 17], "join": 10, "here": 10, "florian": [10, 17], "felten": [10, 17], "ffelten": 10, "luca": [10, 17], "n": [10, 11, 12, 17], "alegr": [10, 17], "lucasalegr": [10, 17], "open": 10, "alwai": [10, 16], "happi": 10, "receiv": 10, "bug": 10, "fix": 10, "featur": [10, 13], "want": 10, "our": 10, "discuss": 10, "your": 10, "u": 10, "also": 10, "issu": 10, "pull": 10, "request": 10, "directli": 10, "asid": 10, "contributor": 10, "mani": 10, "who": 10, "project": 10, "variou": 10, "wai": [10, 11], "would": 10, "like": 10, "thank": 10, "them": 10, "willem": 10, "r\u00f6pke": 10, "hi": 10, "wilrop": 10, "deni": 10, "conor": 10, "f": 10, "hay": 10, "provid": [10, 11, 14, 17], "origin": [10, 17], "librari": [11, 17], "These": 11, "below": 11, "common": [11, 12, 13, 14, 15, 16, 18], "replaybuff": 11, "obs_shap": 11, "action_dim": 11, "rew_dim": 11, "max_siz": 11, "obs_dtyp": 11, "float32": 11, "action_dtyp": 11, "next_ob": 11, "get_all_data": 11, "max_sampl": 11, "data": 11, "replac": 11, "use_c": 11, "to_tensor": 11, "batch": 11, "size": [11, 12], "cer": 11, "convert": 11, "pytorch": [11, 17], "sample_ob": 11, "diverse_buff": 11, "diversememori": 11, "main_capac": 11, "sec_capac": 11, "trace_divers": 11, "crowding_divers": 11, "value_funct": 11, "lambda": 11, "integr": 11, "secondari": 11, "code": [11, 14, 17], "extract": 11, "github": [11, 17], "com": [11, 14, 17], "axelabel": 11, "dynmorl": 11, "error": 11, "trace_id": 11, "pred_idx": 11, "tree_id": 11, "proport": 11, "same": 11, "treat": 11, "trace": 11, "determin": 11, "transit": 11, "store": 11, "identifi": 11, "tree": 11, "relev": 11, "index": 11, "node": 11, "wa": 11, "add_sampl": 11, "write": 11, "add_tre": 11, "dupe": 11, "trg_i": 11, "src_i": 11, "copi": 11, "sourc": [11, 14], "extract_trac": 11, "posit": 11, "those": 11, "get_data": 11, "include_indic": 11, "includ": 11, "get_error": 11, "idx": 11, "correspond": 11, "get_sec_writ": 11, "secondary_trac": 11, "reserved_idx": 11, "find": 11, "free": 11, "spot": 11, "memori": [11, 14], "recurs": 11, "past": 11, "low": 11, "crowd": 11, "distanc": [11, 15], "get_trace_valu": 11, "trace_tupl": 11, "appli": 11, "main_mem_is_ful": 11, "becaus": 11, "circular": 11, "fill": [11, 12], "suffici": 11, "know": 11, "full": 11, "move_to_sec": 11, "span": 11, "remove_trac": 11, "whose": 11, "sec_dist": 11, "prioritized_buff": 11, "prioritizedreplaybuff": 11, "min_prior": [11, 12], "1e": 11, "05": 11, "update_prior": 11, "accrued_reward_buff": 11, "accruedrewardreplaybuff": 11, "action_shap": 11, "cleanup": 11, "whole": 11, "order": 11, "element": [11, 14], "get_grad_norm": 12, "param": 12, "iter": 12, "how": 12, "grad": 12, "norm": 12, "insid": 12, "nn": [12, 13], "clip_grad_norm_": 12, "huber": 12, "loss": 12, "minimum": 12, "layer_init": 12, "layer": [12, 13], "orthogon": 12, "weight_gain": 12, "bias_const": 12, "initi": 12, "gain": 12, "constant": 12, "bia": 12, "linearly_decaying_valu": 12, "initial_valu": 12, "decay_period": 12, "step": 12, "warmup_step": 12, "final_valu": 12, "linearli": 12, "decai": 12, "natur": [12, 13], "schedul": 12, "mnih": [12, 13], "et": [12, 13], "al": [12, 13], "2015": [12, 13], "begin": 12, "until": 12, "been": [12, 14, 17], "taken": 12, "period": 12, "over": 12, "complet": 12, "so": [12, 16], "far": [12, 16], "befor": 12, "accord": 12, "log_episode_info": 12, "global_timestep": 12, "inform": 12, "last": 12, "automat": [12, 16, 17], "recordstatisticswrapp": 12, "statist": 12, "global": 12, "wandb": 12, "polyak_upd": 12, "target_param": 12, "polyak": 12, "averag": [12, 15], "coeffici": 12, "usual": 12, "small": 12, "random_weight": 12, "dim": 12, "dist": 12, "gaussian": 12, "random": 12, "normal": 12, "dirichlet": 12, "distribut": 12, "alpha": 12, "either": 12, "naturecnn": 13, "observation_shap": 13, "features_dim": 13, "512": 13, "cnn": 13, "volodymyr": 13, "human": 13, "level": 13, "through": 13, "deep": 13, "518": 13, "7540": 13, "529": 13, "533": 13, "forward": 13, "mlp": 13, "input_dim": 13, "output_dim": 13, "activation_fn": 13, "modul": 13, "activ": 13, "relu": 13, "sequenti": 13, "creat": 13, "perceptron": 13, "fulli": 13, "connect": 13, "dimens": [13, 16], "output": 13, "architectur": 13, "net": [13, 18], "It": [13, 16, 17], "unit": 13, "length": 13, "after": 13, "paretoarch": 14, "archiv": 14, "candid": 14, "ineffici": 14, "point": [14, 15, 16], "get_non_domin": 14, "subset": 14, "stackoverflow": 14, "32791911": 14, "fast": 14, "calcul": 14, "python": 14, "answer": 14, "wrong": 14, "import": 14, "made": [14, 15], "rl": 15, "mostli": 15, "pymoo": 15, "some": 15, "customli": 15, "performance_ind": 15, "_supportsarrai": 15, "dtype": 15, "_nestedsequ": 15, "complex": 15, "byte": 15, "sparsiti": 15, "basic": 15, "tchebicheff": 16, "reward_dim": 16, "requir": 16, "seen": 16, "sure": 16, "sum": 16, "dot": 16, "product": 16, "aim": 17, "reliabl": 17, "strictli": 17, "gymnasium": 17, "api": 17, "differ": 17, "standard": 17, "onli": 17, "For": 17, "detail": [17, 18], "mdp": 17, "momdp": 17, "suggest": 17, "read": 17, "practic": 17, "plan": 17, "under": 17, "both": 17, "criteria": 17, "perform": 17, "report": 17, "bias": 17, "dashboard": 17, "lint": 17, "format": 17, "enforc": 17, "pre": 17, "commit": 17, "hook": 17, "well": 17, "document": [17, 18], "test": 17, "etc": [17, 18], "against": 17, "ones": 17, "hyper": 17, "misc": 17, "author": 17, "titl": 17, "year": 17, "2022": 17, "publish": 17, "howpublish": 17, "url": 17, "As": 18, "much": 18, "possibl": 18, "repo": 18, "tri": 18, "file": 18, "rule": 18, "structur": 18, "exampl": 18, "gym": 18, "recur": 18, "concept": 18, "neural": 18, "more": 18}, "objects": {"morl_baselines.common.accrued_reward_buffer": [[11, 0, 1, "", "AccruedRewardReplayBuffer"]], "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer": [[11, 1, 1, "", "add"], [11, 1, 1, "", "cleanup"], [11, 1, 1, "", "get_all_data"], [11, 1, 1, "", "sample"]], "morl_baselines.common.buffer": [[11, 0, 1, "", "ReplayBuffer"]], "morl_baselines.common.buffer.ReplayBuffer": [[11, 1, 1, "", "add"], [11, 1, 1, "", "get_all_data"], [11, 1, 1, "", "sample"], [11, 1, 1, "", "sample_obs"]], "morl_baselines.common.diverse_buffer": [[11, 0, 1, "", "DiverseMemory"]], "morl_baselines.common.diverse_buffer.DiverseMemory": [[11, 1, 1, "", "add"], [11, 1, 1, "", "add_sample"], [11, 1, 1, "", "add_tree"], [11, 1, 1, "", "dupe"], [11, 1, 1, "", "extract_trace"], [11, 1, 1, "", "get"], [11, 1, 1, "", "get_data"], [11, 1, 1, "", "get_error"], [11, 1, 1, "", "get_sec_write"], [11, 1, 1, "", "get_trace_value"], [11, 1, 1, "", "main_mem_is_full"], [11, 1, 1, "", "move_to_sec"], [11, 1, 1, "", "remove_trace"], [11, 1, 1, "", "sample"], [11, 1, 1, "", "sec_distances"], [11, 1, 1, "", "update"]], "morl_baselines.common": [[13, 2, 0, "-", "networks"], [14, 2, 0, "-", "pareto"], [15, 2, 0, "-", "performance_indicators"], [16, 2, 0, "-", "scalarization"], [12, 2, 0, "-", "utils"]], "morl_baselines.common.networks": [[13, 0, 1, "", "NatureCNN"], [13, 3, 1, "", "mlp"]], "morl_baselines.common.networks.NatureCNN": [[13, 1, 1, "", "forward"]], "morl_baselines.common.pareto": [[14, 0, 1, "", "ParetoArchive"], [14, 3, 1, "", "get_non_dominated"]], "morl_baselines.common.pareto.ParetoArchive": [[14, 1, 1, "", "add"]], "morl_baselines.common.performance_indicators": [[15, 3, 1, "", "hypervolume"], [15, 3, 1, "", "sparsity"]], "morl_baselines.common.prioritized_buffer": [[11, 0, 1, "", "PrioritizedReplayBuffer"]], "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer": [[11, 1, 1, "", "add"], [11, 1, 1, "", "get_all_data"], [11, 1, 1, "", "sample"], [11, 1, 1, "", "sample_obs"], [11, 1, 1, "", "update_priorities"]], "morl_baselines.common.scalarization": [[16, 3, 1, "", "tchebicheff"], [16, 3, 1, "", "weighted_sum"]], "morl_baselines.common.utils": [[12, 3, 1, "", "get_grad_norm"], [12, 3, 1, "", "huber"], [12, 3, 1, "", "layer_init"], [12, 3, 1, "", "linearly_decaying_value"], [12, 3, 1, "", "log_episode_info"], [12, 3, 1, "", "polyak_update"], [12, 3, 1, "", "random_weights"]], "morl_baselines.multi_policy.envelope.envelope": [[2, 0, 1, "", "Envelope"]], "morl_baselines.multi_policy.envelope.envelope.Envelope": [[2, 1, 1, "", "act"], [2, 1, 1, "", "ddqn_target"], [2, 1, 1, "", "envelope_target"], [2, 1, 1, "", "eval"], [2, 1, 1, "", "get_config"], [2, 1, 1, "", "load"], [2, 1, 1, "", "max_action"], [2, 1, 1, "", "save"], [2, 1, 1, "", "train"], [2, 1, 1, "", "update"]], "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning": [[3, 0, 1, "", "MPMOQLearning"]], "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning": [[3, 1, 1, "", "eval_all_agents"], [3, 1, 1, "", "get_config"], [3, 1, 1, "", "train"]], "morl_baselines.multi_policy.ols.ols": [[4, 0, 1, "", "OLS"]], "morl_baselines.multi_policy.ols.ols.OLS": [[4, 1, 1, "", "add_solution"], [4, 1, 1, "", "compute_corner_weights"], [4, 1, 1, "", "ended"], [4, 1, 1, "", "extrema_weights"], [4, 1, 1, "", "get_ccs_weights"], [4, 1, 1, "", "get_corner_weights"], [4, 1, 1, "", "get_priority"], [4, 1, 1, "", "is_dominated"], [4, 1, 1, "", "max_scalarized_value"], [4, 1, 1, "", "max_value_lp"], [4, 1, 1, "", "next_weight"], [4, 1, 1, "", "remove_obsolete_values"], [4, 1, 1, "", "remove_obsolete_weights"]], "morl_baselines.multi_policy.pareto_q_learning.pql": [[5, 0, 1, "", "PQL"]], "morl_baselines.multi_policy.pareto_q_learning.pql.PQL": [[5, 1, 1, "", "calc_non_dominated"], [5, 1, 1, "", "get_config"], [5, 1, 1, "", "get_local_pcs"], [5, 1, 1, "", "get_q_set"], [5, 1, 1, "", "score_hypervolume"], [5, 1, 1, "", "score_pareto_cardinality"], [5, 1, 1, "", "select_action"], [5, 1, 1, "", "track_policy"], [5, 1, 1, "", "train"]], "morl_baselines.multi_policy.pgmorl.pgmorl": [[6, 0, 1, "", "PGMORL"]], "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL": [[6, 1, 1, "", "get_config"], [6, 1, 1, "", "train"]], "morl_baselines.single_policy.esr.eupg": [[8, 0, 1, "", "EUPG"]], "morl_baselines.single_policy.esr.eupg.EUPG": [[8, 1, 1, "", "eval"], [8, 1, 1, "", "get_config"], [8, 1, 1, "", "train"], [8, 1, 1, "", "update"]], "morl_baselines.single_policy.ser.mo_q_learning": [[9, 0, 1, "", "MOQLearning"]], "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning": [[9, 1, 1, "", "eval"], [9, 1, 1, "", "get_config"], [9, 1, 1, "", "train"], [9, 1, 1, "", "update"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:module", "3": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "module", "Python module"], "3": ["py", "function", "Python function"]}, "titleterms": {"overview": [0, 18], "multi": [1, 11, 17], "polici": [1, 7], "algorithm": [1, 7, 17], "envelop": 2, "q": [2, 5], "learn": [2, 3, 5, 9, 17], "mpmoq": 3, "ol": 4, "pareto": [5, 14], "pgmorl": 6, "singl": 7, "eupg": 8, "moq": 9, "commun": 10, "maintain": 10, "contribut": 10, "acknowledg": 10, "replai": 11, "buffer": 11, "object": [11, 17], "divers": 11, "priorit": 11, "accru": 11, "reward": 11, "miscellan": 12, "neural": 13, "network": 13, "helper": 13, "util": 14, "perform": 15, "indic": 15, "scalar": 16, "function": 16, "morl": 17, "baselin": 17, "A": 17, "collect": 17, "reinforc": 17, "featur": 17, "cite": 17}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"Overview": [[0, "overview"], [18, "overview"]], "Multi-Policy Algorithms": [[1, "multi-policy-algorithms"]], "Envelope Q-Learning": [[2, "envelope-q-learning"]], "MPMOQ Learning": [[3, "mpmoq-learning"]], "OLS": [[4, "ols"]], "Pareto Q-Learning": [[5, "pareto-q-learning"]], "PGMORL": [[6, "pgmorl"]], "Single-policy Algorithms": [[7, "single-policy-algorithms"]], "EUPG": [[8, "eupg"]], "MOQ-Learning": [[9, "moq-learning"]], "Community": [[10, "community"]], "Maintainers": [[10, "maintainers"]], "Contributing": [[10, "contributing"]], "Acknowledgements": [[10, "acknowledgements"]], "Replay Buffers": [[11, "replay-buffers"]], "Multi-Objective Replay Buffer": [[11, "multi-objective-replay-buffer"]], "Diverse Replay Buffer": [[11, "diverse-replay-buffer"]], "Prioritized Replay Buffer": [[11, "prioritized-replay-buffer"]], "Accrued Reward Replay Buffer": [[11, "accrued-reward-replay-buffer"]], "Miscellaneous": [[12, "module-morl_baselines.common.utils"]], "Neural Networks helpers": [[13, "module-morl_baselines.common.networks"]], "Pareto utils": [[14, "module-morl_baselines.common.pareto"]], "Performance indicators": [[15, "module-morl_baselines.common.performance_indicators"]], "Scalarization functions": [[16, "module-morl_baselines.common.scalarization"]], "MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.": [[17, "morl-baselines-a-collection-of-multi-objective-reinforcement-learning-algorithms"]], "Features of MORL-Baselines": [[17, "features-of-morl-baselines"]], "Citing MORL-Baselines": [[17, "citing-morl-baselines"]]}, "indexentries": {"envelope (class in morl_baselines.multi_policy.envelope.envelope)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope"]], "act() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.act"]], "ddqn_target() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.ddqn_target"]], "envelope_target() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.envelope_target"]], "eval() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.eval"]], "get_config() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.get_config"]], "load() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.load"]], "max_action() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.max_action"]], "save() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.save"]], "train() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.train"]], "update() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.update"]], "mpmoqlearning (class in morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning)": [[3, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning"]], "eval_all_agents() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[3, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.eval_all_agents"]], "get_config() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[3, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.get_config"]], "train() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[3, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.train"]], "ols (class in morl_baselines.multi_policy.ols.ols)": [[4, "morl_baselines.multi_policy.ols.ols.OLS"]], "add_solution() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.add_solution"]], "compute_corner_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.compute_corner_weights"]], "ended() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.ended"]], "extrema_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.extrema_weights"]], "get_ccs_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.get_ccs_weights"]], "get_corner_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.get_corner_weights"]], "get_priority() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.get_priority"]], "is_dominated() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.is_dominated"]], "max_scalarized_value() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.max_scalarized_value"]], "max_value_lp() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.max_value_lp"]], "next_weight() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.next_weight"]], "remove_obsolete_values() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.remove_obsolete_values"]], "remove_obsolete_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.remove_obsolete_weights"]], "pql (class in morl_baselines.multi_policy.pareto_q_learning.pql)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL"]], "calc_non_dominated() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.calc_non_dominated"]], "get_config() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_config"]], "get_local_pcs() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_local_pcs"]], "get_q_set() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_q_set"]], "score_hypervolume() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.score_hypervolume"]], "score_pareto_cardinality() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.score_pareto_cardinality"]], "select_action() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.select_action"]], "track_policy() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.track_policy"]], "train() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.train"]], "pgmorl (class in morl_baselines.multi_policy.pgmorl.pgmorl)": [[6, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL"]], "get_config() (morl_baselines.multi_policy.pgmorl.pgmorl.pgmorl method)": [[6, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL.get_config"]], "train() (morl_baselines.multi_policy.pgmorl.pgmorl.pgmorl method)": [[6, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL.train"]], "eupg (class in morl_baselines.single_policy.esr.eupg)": [[8, "morl_baselines.single_policy.esr.eupg.EUPG"]], "eval() (morl_baselines.single_policy.esr.eupg.eupg method)": [[8, "morl_baselines.single_policy.esr.eupg.EUPG.eval"]], "get_config() (morl_baselines.single_policy.esr.eupg.eupg method)": [[8, "morl_baselines.single_policy.esr.eupg.EUPG.get_config"]], "train() (morl_baselines.single_policy.esr.eupg.eupg method)": [[8, "morl_baselines.single_policy.esr.eupg.EUPG.train"]], "update() (morl_baselines.single_policy.esr.eupg.eupg method)": [[8, "morl_baselines.single_policy.esr.eupg.EUPG.update"]], "moqlearning (class in morl_baselines.single_policy.ser.mo_q_learning)": [[9, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning"]], "eval() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[9, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.eval"]], "get_config() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[9, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.get_config"]], "train() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[9, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.train"]], "update() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[9, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.update"]], "accruedrewardreplaybuffer (class in morl_baselines.common.accrued_reward_buffer)": [[11, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer"]], "diversememory (class in morl_baselines.common.diverse_buffer)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory"]], "prioritizedreplaybuffer (class in morl_baselines.common.prioritized_buffer)": [[11, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer"]], "replaybuffer (class in morl_baselines.common.buffer)": [[11, "morl_baselines.common.buffer.ReplayBuffer"]], "add() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[11, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.add"]], "add() (morl_baselines.common.buffer.replaybuffer method)": [[11, "morl_baselines.common.buffer.ReplayBuffer.add"]], "add() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.add"]], "add() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[11, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.add"]], "add_sample() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.add_sample"]], "add_tree() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.add_tree"]], "cleanup() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[11, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.cleanup"]], "dupe() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.dupe"]], "extract_trace() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.extract_trace"]], "get() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.get"]], "get_all_data() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[11, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.get_all_data"]], "get_all_data() (morl_baselines.common.buffer.replaybuffer method)": [[11, "morl_baselines.common.buffer.ReplayBuffer.get_all_data"]], "get_all_data() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[11, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.get_all_data"]], "get_data() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.get_data"]], "get_error() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.get_error"]], "get_sec_write() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.get_sec_write"]], "get_trace_value() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.get_trace_value"]], "main_mem_is_full() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.main_mem_is_full"]], "move_to_sec() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.move_to_sec"]], "remove_trace() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.remove_trace"]], "sample() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[11, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.sample"]], "sample() (morl_baselines.common.buffer.replaybuffer method)": [[11, "morl_baselines.common.buffer.ReplayBuffer.sample"]], "sample() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.sample"]], "sample() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[11, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.sample"]], "sample_obs() (morl_baselines.common.buffer.replaybuffer method)": [[11, "morl_baselines.common.buffer.ReplayBuffer.sample_obs"]], "sample_obs() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[11, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.sample_obs"]], "sec_distances() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.sec_distances"]], "update() (morl_baselines.common.diverse_buffer.diversememory method)": [[11, "morl_baselines.common.diverse_buffer.DiverseMemory.update"]], "update_priorities() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[11, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.update_priorities"]], "get_grad_norm() (in module morl_baselines.common.utils)": [[12, "morl_baselines.common.utils.get_grad_norm"]], "huber() (in module morl_baselines.common.utils)": [[12, "morl_baselines.common.utils.huber"]], "layer_init() (in module morl_baselines.common.utils)": [[12, "morl_baselines.common.utils.layer_init"]], "linearly_decaying_value() (in module morl_baselines.common.utils)": [[12, "morl_baselines.common.utils.linearly_decaying_value"]], "log_episode_info() (in module morl_baselines.common.utils)": [[12, "morl_baselines.common.utils.log_episode_info"]], "module": [[12, "module-morl_baselines.common.utils"], [13, "module-morl_baselines.common.networks"], [14, "module-morl_baselines.common.pareto"], [15, "module-morl_baselines.common.performance_indicators"], [16, "module-morl_baselines.common.scalarization"]], "morl_baselines.common.utils": [[12, "module-morl_baselines.common.utils"]], "polyak_update() (in module morl_baselines.common.utils)": [[12, "morl_baselines.common.utils.polyak_update"]], "random_weights() (in module morl_baselines.common.utils)": [[12, "morl_baselines.common.utils.random_weights"]], "naturecnn (class in morl_baselines.common.networks)": [[13, "morl_baselines.common.networks.NatureCNN"]], "forward() (morl_baselines.common.networks.naturecnn method)": [[13, "morl_baselines.common.networks.NatureCNN.forward"]], "mlp() (in module morl_baselines.common.networks)": [[13, "morl_baselines.common.networks.mlp"]], "morl_baselines.common.networks": [[13, "module-morl_baselines.common.networks"]], "paretoarchive (class in morl_baselines.common.pareto)": [[14, "morl_baselines.common.pareto.ParetoArchive"]], "add() (morl_baselines.common.pareto.paretoarchive method)": [[14, "morl_baselines.common.pareto.ParetoArchive.add"]], "get_non_dominated() (in module morl_baselines.common.pareto)": [[14, "morl_baselines.common.pareto.get_non_dominated"]], "morl_baselines.common.pareto": [[14, "module-morl_baselines.common.pareto"]], "hypervolume() (in module morl_baselines.common.performance_indicators)": [[15, "morl_baselines.common.performance_indicators.hypervolume"]], "morl_baselines.common.performance_indicators": [[15, "module-morl_baselines.common.performance_indicators"]], "sparsity() (in module morl_baselines.common.performance_indicators)": [[15, "morl_baselines.common.performance_indicators.sparsity"]], "morl_baselines.common.scalarization": [[16, "module-morl_baselines.common.scalarization"]], "tchebicheff() (in module morl_baselines.common.scalarization)": [[16, "morl_baselines.common.scalarization.tchebicheff"]], "weighted_sum() (in module morl_baselines.common.scalarization)": [[16, "morl_baselines.common.scalarization.weighted_sum"]]}})