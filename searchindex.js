Search.setIndex({"docnames": ["algos/algorithms", "algos/multi_policy", "algos/multi_policy/capql", "algos/multi_policy/envelope", "algos/multi_policy/gpi_pd", "algos/multi_policy/linear_support", "algos/multi_policy/mp_mo_q_learning", "algos/multi_policy/pareto_q_learning", "algos/multi_policy/pcn", "algos/multi_policy/pgmorl", "algos/performances", "algos/single_policy", "algos/single_policy/eupg", "algos/single_policy/moq_learning", "community/community", "features/buffers", "features/evaluations", "features/misc", "features/networks", "features/pareto", "features/performance_indicators", "features/scalarization", "features/weights", "index", "quickstart/overview"], "filenames": ["algos/algorithms.md", "algos/multi_policy.md", "algos/multi_policy/capql.md", "algos/multi_policy/envelope.md", "algos/multi_policy/gpi_pd.md", "algos/multi_policy/linear_support.md", "algos/multi_policy/mp_mo_q_learning.md", "algos/multi_policy/pareto_q_learning.md", "algos/multi_policy/pcn.md", "algos/multi_policy/pgmorl.md", "algos/performances.md", "algos/single_policy.md", "algos/single_policy/eupg.md", "algos/single_policy/moq_learning.md", "community/community.md", "features/buffers.md", "features/evaluations.md", "features/misc.md", "features/networks.md", "features/pareto.md", "features/performance_indicators.md", "features/scalarization.md", "features/weights.md", "index.md", "quickstart/overview.md"], "titles": ["Overview", "Multi-Policy Algorithms", "Concave-Augmented Pareto Q-Learning (CAPQL)", "Envelope Q-Learning", "GPI-Prioritized Dyna", "Linear Support", "MPMOQ Learning", "Pareto Q-Learning", "Pareto Conditioned Networks", "PGMORL", "Performance assessments", "Single-policy Algorithms", "EUPG", "MOQ-Learning", "Community", "Replay Buffers", "Evaluations", "Miscellaneous", "Neural Networks helpers", "Pareto utils", "Performance indicators", "Scalarization functions", "Weights helpers", "MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.", "Overview"], "terms": {"morl": [0, 2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, 17, 20, 24], "baselin": [0, 2, 3, 4, 6, 7, 8, 9, 12, 13, 14, 17, 24], "contain": [0, 15, 16, 23, 24], "multipl": [0, 3, 9, 15, 16], "implement": [0, 5, 9, 10, 14, 15, 23, 24], "multi": [0, 2, 3, 4, 6, 7, 9, 12, 13, 16, 18, 20, 24], "object": [0, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 16, 20], "reinforc": [0, 2, 3, 6, 7, 9, 10, 12, 13, 15, 18], "learn": [0, 4, 9, 10, 12, 14, 15, 18], "algorithm": [0, 2, 3, 4, 5, 6, 9, 12, 13, 14, 15, 19, 20, 24], "The": [0, 3, 5, 6, 7, 9, 10, 12, 15, 17, 18, 19, 23, 24], "follow": [0, 6, 10, 15, 17, 18, 23, 24], "tabl": [0, 10, 13], "list": [0, 2, 3, 4, 5, 6, 7, 8, 9, 12, 15, 16, 17, 18, 19, 20, 22], "ar": [0, 5, 9, 10, 14, 15, 19, 20, 22, 23], "current": [0, 3, 5, 7, 9, 10, 14, 15, 16, 17, 18, 20], "name": [0, 7], "singl": [0, 13, 23, 24], "polici": [0, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 16, 20, 23, 24], "esr": [0, 12, 15, 16, 23, 24], "ser": [0, 9, 13, 23, 24], "observ": [0, 2, 3, 8, 9, 12, 13, 15, 18], "space": [0, 9, 10, 22], "action": [0, 2, 3, 4, 6, 7, 8, 9, 12, 13, 15], "paper": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 18, 20], "gpi": [0, 5, 6], "l": [0, 4, 5, 10, 20], "pd": [0, 4], "continu": [0, 9, 10], "discret": 0, "supplementari": [0, 9], "materi": [0, 9], "envelop": 0, "q": [0, 6, 13, 14], "capql": 0, "pgmorl": [0, 10, 20], "1": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 15, 17, 18, 22], "pareto": [0, 3, 4, 6, 9, 10, 14, 16, 20, 23], "condit": [0, 3, 12, 16], "network": [0, 3, 4, 9, 12], "pcn": [0, 8], "2": [0, 2, 4, 9, 10, 15], "mo": [0, 6, 10, 13, 16, 23, 24], "mpmoqlearn": [0, 6], "outer": [0, 6], "loop": [0, 6], "moql": 0, "optimist": [0, 5], "linear": 0, "support": [0, 4, 9, 10, 23], "ol": [0, 5], "section": [0, 5, 9], "3": [0, 5, 7, 9, 10, 23], "thesi": [0, 5], "expect": [0, 10, 12, 16, 20], "util": [0, 5, 10, 12, 16, 17, 18, 20, 22, 23], "gradient": [0, 12], "eupg": [0, 14], "warn": [0, 10], "some": [0, 9, 10, 20, 23], "them": [0, 10, 14], "have": [0, 9, 10, 14, 17, 19, 23], "limit": 0, "featur": [0, 14, 18], "i": [0, 3, 5, 6, 8, 9, 10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], "environ": [0, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 16, 23, 24], "assum": 0, "determinist": 0, "transit": [0, 15], "class": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 15, 18, 19], "morl_baselin": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23], "multi_polici": [2, 3, 4, 5, 6, 7, 8, 9, 24], "env": [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 16, 17], "learning_r": [2, 3, 4, 6, 8, 9, 12, 13], "float": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 15, 16, 17, 18, 20, 21], "0": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 15, 17, 18, 22], "0003": [2, 3, 4, 9], "gamma": [2, 3, 4, 6, 7, 8, 9, 12, 13], "99": [2, 3, 4, 12], "tau": [2, 3, 4, 18, 21], "005": 2, "buffer_s": [2, 3, 4, 12], "int": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 15, 16, 17, 18, 20, 21, 22], "1000000": [2, 3, 4], "net_arch": [2, 3, 4, 9, 12, 18], "256": [2, 3, 4, 8], "batch_siz": [2, 3, 4, 8, 15], "128": [2, 4], "num_q_net": 2, "alpha": [2, 13, 22], "learning_start": [2, 3, 4, 13], "1000": [2, 4, 6, 7, 12, 13], "gradient_upd": [2, 3, 4], "project_nam": [2, 3, 4, 6, 7, 8, 9, 12, 13], "str": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 17, 20, 22], "experiment_nam": [2, 3, 4, 6, 7, 8, 9, 12, 13], "wandb_ent": [2, 3, 4, 6, 7, 8, 9, 12, 13], "none": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 15, 16, 18, 22], "log": [2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 16], "bool": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 15, 16, 18, 19, 20], "true": [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 16, 19, 20], "seed": [2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 16, 22], "devic": [2, 3, 4, 8, 9, 12, 15], "auto": [2, 3, 4, 8, 9, 12], "convex": [2, 10, 19], "stationar": 2, "AND": 2, "optim": [2, 4, 5, 6, 7, 8, 10, 20, 23], "haoy": 2, "lu": 2, "daniel": 2, "herman": 2, "yaoliang": 2, "yu": 2, "iclr": 2, "2023": [2, 4], "http": [2, 3, 4, 5, 8, 9, 10, 15, 19, 22, 23], "openreview": 2, "net": [2, 9, 18, 24], "pdf": [2, 5, 8, 9], "id": [2, 9, 13, 15, 16], "tjezisyesq6": 2, "code": [2, 8, 9, 15, 19, 23], "base": [2, 7, 9, 10, 12, 20], "github": [2, 8, 9, 15, 23], "com": [2, 8, 9, 15, 19, 23], "haoyelu": 2, "eval": [2, 3, 4, 6, 8, 9, 10, 12, 13], "ob": [2, 3, 4, 5, 6, 8, 9, 12, 13, 15], "ndarrai": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 16, 17, 18, 19, 20, 21, 22], "tensor": [2, 3, 4, 15, 18], "w": [2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 16], "torch_act": 2, "fals": [2, 3, 4, 5, 6, 9, 13, 15, 16, 18, 19], "evalu": [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 19], "given": [2, 3, 4, 6, 7, 8, 9, 12, 13, 15, 18], "weight": [2, 3, 4, 5, 6, 8, 10, 12, 13, 16, 17, 18, 20, 21, 23], "vector": [2, 3, 4, 5, 7, 9, 16, 18, 19, 20, 21, 22], "get_config": [2, 3, 4, 6, 7, 8, 9, 12, 13], "get": [2, 5, 6, 7, 8, 14, 15], "configur": [2, 3, 4, 6, 7, 8, 9, 10, 12, 13], "agent": [2, 3, 4, 5, 8, 9, 10, 12, 13, 16, 17], "load": [2, 3, 4], "path": [2, 3, 4], "load_replay_buff": [2, 3, 4], "from": [2, 3, 5, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 20, 22, 23], "file": [2, 24], "save": [2, 3, 4, 8, 17], "save_dir": [2, 3, 4], "filenam": [2, 3, 4, 8], "save_replay_buff": [2, 3, 4], "": [2, 3, 9, 10, 12, 15, 16, 22, 23, 24], "replai": [2, 3, 4, 8, 24], "buffer": [2, 3, 4, 8, 9, 12, 23, 24], "train": [2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17], "total_timestep": [2, 3, 4, 6, 7, 8, 9, 12, 13], "eval_env": [2, 3, 4, 6, 7, 8, 9, 12, 13], "ref_point": [2, 3, 4, 6, 7, 8, 9, 20], "known_pareto_front": [2, 3, 4, 6, 7, 8, 9], "num_eval_weights_for_front": [2, 3, 4, 6], "100": [2, 3, 4, 6, 8, 9], "num_eval_episodes_for_front": [2, 3, 4, 6], "5": [2, 3, 4, 6, 9, 13, 16], "eval_freq": [2, 3, 4, 6, 12, 13], "reset_num_timestep": [2, 3, 4, 13], "paramet": [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23], "total": [2, 3, 6, 8], "number": [2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 22], "timestep": [2, 3, 4, 6, 7, 12, 13, 16], "gym": [2, 4, 7], "us": [2, 3, 4, 5, 6, 7, 9, 10, 12, 15, 16, 17, 18, 20, 22, 24], "np": [2, 3, 4, 5, 12, 13, 16, 17, 20], "refer": [2, 3, 4, 6, 7, 8, 9, 16, 20, 21], "point": [2, 3, 4, 6, 7, 8, 10, 16, 19, 20, 21], "hypervolum": [2, 3, 4, 6, 7, 8, 10, 16, 20], "calcul": [2, 4, 6, 8, 19], "option": [2, 3, 4, 5, 7, 12, 13, 16, 19], "front": [2, 3, 4, 6, 7, 8, 10, 16, 19, 20], "known": [2, 3, 4, 6, 7, 8, 16, 20], "episod": [2, 3, 4, 6, 7, 8, 12, 16, 17], "run": [2, 3, 4, 6, 10, 12, 16, 23], "when": [2, 3, 4, 5, 6, 9, 10, 13, 15], "between": [2, 4, 10, 13, 20], "dure": [2, 7, 9], "an": [2, 3, 4, 5, 7, 10, 14, 16, 17, 18, 23], "iter": [2, 4, 6, 9, 18], "whether": [2, 3, 4, 13, 15, 16, 18, 19], "reset": [2, 3, 4, 13], "updat": [2, 3, 4, 8, 9, 12, 13, 15, 18], "initial_epsilon": [3, 4, 6, 7, 13], "01": [3, 4, 15, 18], "final_epsilon": [3, 4, 6, 7, 13], "epsilon_decay_step": [3, 4, 6, 7, 13], "target_net_update_freq": [3, 4], "200": 3, "max_grad_norm": [3, 4, 9], "num_sample_w": 3, "4": [3, 9], "per": [3, 4, 6, 7, 8, 13, 16, 18], "per_alpha": 3, "6": [3, 4, 9, 13], "initial_homotopy_lambda": 3, "final_homotopy_lambda": 3, "homotopy_decay_step": 3, "lean": 3, "emb": 3, "take": [3, 16], "input": [3, 18, 19], "main": [3, 9, 14, 15, 16], "chang": [3, 4, 9, 10, 19], "thi": [3, 8, 9, 10, 14, 15, 16, 17, 18, 19, 21, 23, 24], "compar": 3, "scalar": [3, 5, 6, 9, 10, 12, 13, 16], "cn": 3, "dqn": [3, 17, 18], "target": [3, 15, 18], "r": [3, 20], "yang": 3, "x": [3, 18], "sun": 3, "k": [3, 6, 7, 13], "narasimhan": 3, "A": [3, 6, 7, 8, 9, 10, 12, 13, 15, 17, 19, 20], "gener": [3, 4, 5, 6, 10, 12, 13, 16, 17, 20, 22], "adapt": [3, 9, 21], "arxiv": [3, 4, 5], "1908": 3, "08342": 3, "c": [3, 4, 10], "nov": [3, 9, 10], "2019": 3, "access": 3, "sep": 3, "06": 3, "2021": 3, "onlin": 3, "avail": [3, 8, 9, 10, 15, 23], "org": [3, 4, 5, 8, 22], "ab": [3, 4, 5], "act": 3, "epsilon": [3, 5, 17], "greedili": 3, "select": [3, 4, 7, 9], "return": [3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23], "integ": 3, "repres": [3, 18], "ddqn_target": 3, "doubl": 3, "envelope_target": 3, "sampled_w": 3, "comput": [3, 5, 7, 9, 10, 15, 16, 17, 18, 19, 20], "set": [3, 4, 5, 6, 7, 8, 10, 15, 16, 19, 20, 23, 24], "sampl": [3, 4, 8, 9, 15, 16, 22], "give": [3, 12, 13, 15], "best": [3, 6, 9, 12, 13, 21], "arrai": [3, 6, 9, 12, 13, 15, 17, 19, 23], "dictionari": [3, 6, 7, 9, 12, 13, 15, 16], "dict": [3, 6, 7, 8, 9, 12, 13, 16], "config": [3, 6, 9, 12, 13], "model": [3, 4, 8, 13], "specifi": [3, 15], "too": 3, "max_act": [3, 4], "highest": [3, 5], "valu": [3, 5, 6, 7, 9, 10, 13, 15, 16, 17, 20, 21], "directori": 3, "total_episod": 3, "10000": [3, 4, 7], "reset_learning_start": [3, 4], "If": [3, 5, 6, 7, 14, 15, 16], "ignor": 3, "randomli": 3, "everi": [3, 7], "done": [3, 15], "time": [3, 5, 8, 9, 13], "frequenc": [3, 6, 12], "step": [3, 8, 16, 17], "creat": [3, 18], "start": [3, 4, 9, 13, 15], "e": [3, 8, 9, 12, 15, 16, 20, 23], "g": [3, 9, 12, 20, 23], "experi": [3, 8, 9, 12, 15, 23], "gpi_pd": [4, 6, 13], "gpipd": 4, "type": [4, 18, 20], "num_net": 4, "20": [4, 8, 9], "use_gpi": [4, 6], "alpha_p": 4, "min_prior": [4, 13, 15, 18], "drop_rat": [4, 18], "layer_norm": [4, 18], "dynamics_normalize_input": 4, "dynamics_uncertainty_threshold": 4, "dynamics_train_freq": 4, "callabl": [4, 7, 20, 21], "function": [4, 5, 6, 7, 9, 10, 13, 15, 16, 18, 19, 20, 23], "lambda": [4, 15], "dynamics_rollout_len": 4, "dynamics_rollout_start": 4, "5000": 4, "dynamics_rollout_freq": 4, "250": 4, "dynamics_rollout_batch_s": 4, "25000": 4, "dynamics_buffer_s": 4, "100000": [4, 7, 12, 15], "dynamics_net_arch": 4, "dynamics_ensemble_s": 4, "dynamics_num_elit": 4, "real_ratio": 4, "torch": [4, 18], "effici": 4, "via": 4, "improv": [4, 5], "luca": [4, 14, 23], "n": [4, 8, 14, 15, 22, 23], "alegr": [4, 14, 23], "ana": 4, "bazzan": 4, "diederik": 4, "m": [4, 6, 8, 10, 13, 20], "roijer": [4, 5, 10, 12, 20], "ann": 4, "now\u00e9": [4, 7, 8], "bruno": 4, "da": 4, "silva": 4, "aama": 4, "2301": [4, 5], "07784": [4, 5], "gpi_act": 4, "return_policy_index": 4, "include_w": 4, "greedi": [4, 13], "set_weight_support": 4, "weight_list": 4, "timesteps_per_it": 4, "weight_selection_algo": [4, 6], "train_iter": 4, "weight_support": 4, "change_w_every_episod": 4, "one": [4, 13, 16, 20, 22], "end": [4, 5, 15], "each": [4, 9, 10, 13, 15, 18, 20, 21], "linear_support": 5, "linearsupport": 5, "num_object": 5, "verbos": [5, 16], "corner": 5, "both": [5, 10, 23], "info": [5, 16], "pub": 5, "add_solut": 5, "add": [5, 9, 15, 19], "new": [5, 9, 14, 15], "indic": [5, 6, 15, 19], "remov": [5, 15, 19], "cc": [5, 10], "being": 5, "domin": [5, 7, 19, 21], "compute_corner_weight": 5, "see": [5, 9, 24], "definit": [5, 23], "19": 5, "typo": 5, "sign": 5, "should": [5, 15, 16, 19], "queue": 5, "empti": 5, "get_corner_weight": 5, "top_k": 5, "get_weight_support": 5, "gpi_ls_prior": 5, "gpi_expanded_set": 5, "prioriti": [5, 15, 18], "is_domin": 5, "check": [5, 15], "ani": [5, 20], "otherwis": [5, 6], "max_scalarized_valu": 5, "maximum": [5, 6, 8, 9, 10, 15, 16, 20], "max_value_lp": 5, "w_new": 5, "upper": 5, "bound": 5, "next_weight": 5, "algo": 5, "gpi_ag": 5, "mopolici": 5, "rep_ev": 5, "next": [5, 9, 15], "either": [5, 22], "ols_prior": 5, "remove_obsolete_valu": 5, "which": [5, 10, 15, 17, 18, 19, 23], "longer": 5, "after": [5, 9, 18], "ad": 5, "remove_obsolete_weight": 5, "new_valu": 5, "better": 5, "than": [5, 9], "previou": [5, 15], "multi_policy_moqlearn": 6, "mp_mo_q_learn": 6, "weighted_sum": [6, 13, 21], "9": [6, 13], "random": [6, 13, 16, 22], "epsilon_ol": 6, "use_gpi_polici": [6, 13], "transfer_q_t": 6, "dyna": [6, 13], "dyna_upd": [6, 13], "multipolici": 6, "moq": 6, "version": [6, 19], "mo_q_learn": [6, 13], "van": [6, 7, 13], "moffaert": [6, 7, 13], "drugan": [6, 13], "now": [6, 9, 12, 13], "novel": [6, 13], "design": [6, 13, 23], "techniqu": [6, 13], "2013": [6, 13], "doi": [6, 10, 13], "10": [6, 8, 9, 10, 13], "1109": [6, 13], "adprl": [6, 13], "6615007": [6, 13], "delete_polici": 6, "delete_indx": 6, "delet": 6, "choos": [6, 13], "max_scalar_q_valu": 6, "state": [6, 7, 16, 20], "over": [6, 17], "all": [6, 10, 14, 15, 16, 19, 23, 24], "timesteps_per_iter": 6, "200000": 6, "metric": [6, 7, 8, 16, 20], "construct": 6, "pareto_q_learn": 7, "pql": 7, "8": 7, "tabular": 7, "method": [7, 10, 15, 18, 22], "reli": [7, 9, 10, 13, 20, 21], "prune": [7, 19, 23], "journal": [7, 23], "machin": [7, 9, 10], "research": 7, "vol": [7, 10], "15": 7, "pp": [7, 8, 9, 10], "3483": 7, "3512": 7, "2014": 7, "calc_non_domin": 7, "non": [7, 19], "get_local_pc": 7, "collect": [7, 9, 18], "local": 7, "pc": 7, "default": [7, 10, 15, 16, 19, 20, 22], "get_q_set": 7, "pair": [7, 15], "score_hypervolum": 7, "score": 7, "upon": 7, "score_pareto_cardin": 7, "cardin": 7, "select_act": 7, "score_func": 7, "track_polici": 7, "vec": 7, "tol": [7, 17], "001": [7, 8, 12], "track": [7, 10, 23], "its": [7, 15, 19, 20], "array_lik": 7, "toler": [7, 17], "1e": [7, 15], "log_everi": 7, "action_ev": 7, "eval_ref_point": 7, "same": [7, 15], "ref": 7, "result": [7, 10, 23], "final": [7, 10, 17], "scaling_factor": 8, "hidden_dim": 8, "64": [8, 9], "reymond": 8, "bargiacchi": 8, "2022": [8, 10, 23], "mai": 8, "In": [8, 10], "proceed": [8, 9, 10], "21st": 8, "intern": [8, 9, 10], "confer": [8, 9, 10], "autonom": [8, 10], "multiag": 8, "system": [8, 10], "1110": 8, "1118": 8, "www": 8, "ifaama": 8, "aamas2022": 8, "p1110": 8, "credit": 8, "refactor": [8, 9], "author": [8, 9, 23], "mathieu": 8, "max_return": 8, "pcn_model": 8, "savedir": 8, "set_desired_return_and_horizon": 8, "desired_return": 8, "desired_horizon": 8, "desir": 8, "horizon": 8, "num_er_episod": 8, "num_step_episod": 8, "num_model_upd": 8, "50": [8, 10, 12, 16, 17], "max_buffer_s": 8, "num_points_pf": 8, "fill": [8, 15, 16], "clip": 8, "size": [8, 15, 22], "ha": [9, 16], "been": [9, 17, 19, 23], "origin": [9, 10, 14], "provid": [9, 14, 15, 16, 19, 23], "post": 9, "process": [9, 16], "phase": 9, "analysi": [9, 10], "stage": 9, "yet": 9, "ppo": 9, "look": 9, "variou": [9, 10, 14, 20, 23], "tradeoff": 9, "keep": 9, "popul": 9, "along": [9, 10], "perform": [9, 23], "At": 9, "few": 9, "assign": 9, "further": 9, "histor": 9, "data": [9, 15], "gather": 9, "our": [9, 10, 14, 23], "essenti": 9, "cleanrl": [9, 23], "differ": [9, 23], "sum": [9, 21], "note": 9, "might": 9, "possibl": [9, 10, 24], "enhanc": 9, "someth": 9, "els": 9, "single_polici": [9, 12, 13, 24], "mo_ppo": 9, "mopponet": 9, "syncvectorenv": 9, "steps_per_iter": 9, "2048": 9, "num_minibatch": 9, "32": 9, "update_epoch": 9, "995": 9, "anneal_lr": 9, "clip_coef": 9, "ent_coef": 9, "vf_coef": 9, "clip_vloss": 9, "norm_adv": 9, "target_kl": 9, "gae": 9, "gae_lambda": 9, "95": 9, "42": [9, 22], "rng": [9, 22], "modifi": 9, "appli": [9, 15], "clean": 9, "rl": [9, 20, 23], "vwxyzjn": 9, "blob": 9, "master": 9, "ppo_continuous_act": 9, "py": [9, 10], "change_weight": 9, "new_weight": 9, "numpi": [9, 13, 15, 16, 19, 20, 21, 23], "start_tim": [9, 13], "current_iter": 9, "max_iter": 9, "self": 9, "num_env": 9, "more": [9, 24], "detail": [9, 23, 24], "performancepredictor": 9, "neighborhood_threshold": 9, "sigma": 9, "03": 9, "a_bound_min": 9, "a_bound_max": 9, "500": 9, "f_scale": 9, "store": [9, 15], "delta": 9, "Then": 9, "regress": 9, "eval_before_pg": 9, "eval_after_pg": 9, "predictor": 9, "befor": [9, 17], "predict_next_evalu": 9, "weight_candid": 9, "policy_ev": 9, "tupl": [9, 15, 16], "part": 9, "determin": [9, 15], "neighborhood": 9, "threshold": 9, "whose": [9, 15], "candid": [9, 19], "env_id": 9, "pop_siz": 9, "warmup_iter": 9, "80": 9, "evolutionary_iter": 9, "num_weight_candid": 9, "7": 9, "num_performance_buff": 9, "performance_buffer_s": 9, "min_weight": 9, "max_weight": 9, "delta_weight": 9, "guid": [9, 10, 23], "j": [9, 10], "xu": [9, 10], "y": [9, 10], "tian": [9, 10], "p": [9, 10, 20], "ma": [9, 10], "d": [9, 10, 12, 20], "ru": [9, 10], "sueda": [9, 10], "matusik": [9, 10], "robot": [9, 10], "control": [9, 10, 18], "37th": [9, 10], "2020": [9, 10], "10607": [9, 10], "10616": [9, 10], "mlr": [9, 10], "press": [9, 10], "v119": [9, 10], "xu20h": [9, 10], "html": [9, 10, 22], "peopl": [9, 14], "csail": 9, "mit": 9, "edu": 9, "jiex": 9, "supp": 9, "document": [10, 23, 24], "work": 10, "progress": 10, "To": 10, "ensur": 10, "correct": 10, "we": [10, 14, 15, 20, 23], "want": [10, 14], "test": [10, 23], "For": [10, 23], "sake": 10, "reproduc": [10, 16, 23], "mainten": 10, "purpos": 10, "long": 10, "term": 10, "conduct": 10, "gymnasium": [10, 16, 23, 24], "henc": 10, "abl": 10, "were": 10, "present": 10, "keyword": 10, "scalarized_return": 10, "scalarized_discounted_return": 10, "propos": 10, "qualiti": [10, 20], "discount": [10, 16], "pf": [10, 20], "coverag": [10, 19], "converg": 10, "divers": 10, "hybrid": 10, "common": [10, 13, 15, 16, 17, 18, 19, 20, 21, 22, 24], "performance_ind": [10, 20], "sparsiti": [10, 16, 20], "averag": [10, 16, 18, 20], "distanc": [10, 15, 16, 20], "consecut": 10, "igd": [10, 16, 20], "sota": 10, "moo": [10, 20], "literatur": 10, "It": [10, 16, 18, 21, 22, 23], "requir": [10, 20, 21], "can": [10, 14, 23], "posteriori": 10, "That": [10, 22], "do": 10, "merg": 10, "found": [10, 23], "respect": 10, "moreov": 10, "assumpt": [10, 16], "user": 10, "These": [10, 15], "allow": 10, "idea": [10, 12, 14], "wherea": 10, "other": [10, 13, 15, 23], "eum": [10, 16, 20], "mul": [10, 16, 20], "loss": [10, 16, 18, 20], "problem": [10, 20], "know": [10, 15, 20], "equal": [10, 22], "simplex": [10, 22], "also": [10, 14, 16, 23], "wandb": [10, 23], "here": [10, 14, 23], "offici": 10, "sent": 10, "openrlbenchmark": [10, 23], "api": [10, 23], "queri": 10, "plot": 10, "format": [10, 23], "life": 10, "good": 10, "full": [10, 15], "flow": 10, "autom": 10, "cli": 10, "accordingli": 10, "locat": 10, "launch_experi": 10, "below": [10, 15], "issu": [10, 14, 23], "predict": [10, 18], "hay": [10, 14], "et": [10, 17, 18], "al": [10, 17, 18], "practic": [10, 23], "plan": [10, 23], "36": 10, "apr": 10, "1007": 10, "s10458": 10, "022": 10, "09552": 10, "zintgraf": [10, 20], "t": [10, 20], "v": [10, 20], "kanter": [10, 20], "f": [10, 14, 20], "oliehoek": [10, 20], "beau": [10, 20], "approach": [10, 20], "2015": [10, 17, 18, 20], "accru": [12, 16], "reward": [12, 16, 21, 23], "futur": 12, "steckelmach": [12, 14], "2018": 12, "accrued_reward": [12, 15], "moqlearn": 13, "model_bas": 13, "tabular_model": 13, "tabularmodel": 13, "0001": [13, 17], "parent": 13, "parent_rng": 13, "_gener": 13, "maintain": 13, "move": [13, 15], "scalarized_q_valu": 13, "500000": 13, "max": 13, "recal": 13, "launch": 13, "discord": 14, "server": 14, "where": 14, "you": 14, "ask": 14, "question": [14, 19], "help": 14, "repositori": [14, 23], "join": 14, "florian": [14, 23], "felten": [14, 23], "ffelten": 14, "lucasalegr": [14, 23], "open": [14, 23], "alwai": [14, 21], "happi": 14, "receiv": 14, "bug": 14, "fix": 14, "discuss": 14, "your": 14, "u": 14, "pull": 14, "request": 14, "directli": 14, "asid": 14, "contributor": 14, "mani": 14, "who": 14, "project": 14, "wai": [14, 15], "would": 14, "like": 14, "thank": 14, "willem": 14, "r\u00f6pke": 14, "hi": 14, "wilrop": 14, "deni": 14, "conor": 14, "librari": [15, 23], "replaybuff": 15, "obs_shap": 15, "action_dim": 15, "rew_dim": 15, "max_siz": 15, "obs_dtyp": 15, "float32": 15, "action_dtyp": 15, "next_ob": 15, "get_all_data": 15, "max_sampl": 15, "replac": 15, "use_c": 15, "to_tensor": 15, "batch": [15, 19], "cer": 15, "convert": 15, "pytorch": [15, 23], "sample_ob": 15, "diverse_buff": 15, "diversememori": 15, "main_capac": 15, "sec_capac": 15, "trace_divers": 15, "crowding_divers": 15, "value_funct": 15, "integr": 15, "secondari": 15, "extract": 15, "axelabel": 15, "dynmorl": 15, "error": 15, "trace_id": 15, "pred_idx": 15, "tree_id": 15, "proport": 15, "treat": 15, "trace": 15, "identifi": 15, "tree": 15, "relev": 15, "index": 15, "node": 15, "wa": 15, "add_sampl": 15, "write": 15, "add_tre": 15, "dupe": 15, "trg_i": 15, "src_i": 15, "copi": 15, "sourc": [15, 19], "extract_trac": 15, "posit": 15, "those": 15, "get_data": 15, "include_indic": 15, "includ": 15, "get_error": 15, "idx": 15, "correspond": 15, "get_sec_writ": 15, "secondary_trac": 15, "reserved_idx": 15, "find": 15, "free": 15, "spot": 15, "memori": [15, 19], "recurs": 15, "past": 15, "low": 15, "crowd": 15, "get_trace_valu": 15, "trace_tupl": 15, "main_mem_is_ful": 15, "becaus": 15, "circular": 15, "suffici": 15, "move_to_sec": 15, "span": 15, "remove_trac": 15, "sec_dist": 15, "prioritized_buff": 15, "prioritizedreplaybuff": 15, "05": 15, "update_prior": 15, "accrued_reward_buff": 15, "accruedrewardreplaybuff": 15, "action_shap": 15, "cleanup": 15, "whole": 15, "order": 15, "element": [15, 17, 19, 22], "relat": [16, 22], "eval_mo": 16, "dot": [16, 20, 21], "render": [16, 17], "linearreward": 16, "wrapper": 16, "eval_mo_reward_condit": 16, "make": 16, "log_all_multi_policy_metr": 16, "current_front": 16, "hv_ref_point": 16, "reward_dim": [16, 21], "global_step": 16, "n_sample_weight": 16, "ref_front": 16, "invert": [16, 20], "approxim": [16, 20], "global": 16, "log_episode_info": 16, "global_timestep": 16, "inform": 16, "last": 16, "automat": [16, 21, 23], "recordstatisticswrapp": 16, "statist": 16, "print": 16, "policy_evaluation_mo": 16, "rep": 16, "avg": 16, "seed_everyth": 16, "call": 16, "onli": [16, 20, 23], "onc": 16, "python": [16, 19], "prefer": 16, "begin": [16, 17], "script": 16, "effect": 16, "so": [16, 17, 21], "care": 16, "linearly_decaying_valu": 17, "initial_valu": 17, "decay_period": 17, "warmup_step": 17, "final_valu": 17, "linearli": 17, "decai": 17, "natur": [17, 18], "schedul": 17, "mnih": [17, 18], "until": 17, "taken": 17, "period": 17, "complet": 17, "far": [17, 21], "accord": 17, "make_gif": 17, "fullpath": 17, "fp": 17, "length": [17, 18], "300": 17, "gif": 17, "unique_tol": 17, "uniqu": 17, "within": 17, "naturecnn": 18, "observation_shap": 18, "features_dim": 18, "512": 18, "cnn": 18, "volodymyr": 18, "human": 18, "level": 18, "through": 18, "deep": 18, "518": 18, "7540": 18, "529": 18, "533": 18, "forward": 18, "get_grad_norm": 18, "param": 18, "how": 18, "grad": 18, "norm": 18, "insid": 18, "nn": 18, "clip_grad_norm_": 18, "huber": 18, "minimum": 18, "layer_init": 18, "layer": 18, "orthogon": 18, "weight_gain": 18, "bias_const": 18, "initi": 18, "gain": 18, "constant": 18, "bia": 18, "mlp": 18, "input_dim": 18, "output_dim": 18, "activation_fn": 18, "modul": 18, "activ": 18, "relu": 18, "sequenti": 18, "perceptron": 18, "fulli": 18, "connect": 18, "dimens": [18, 21], "output": 18, "architectur": 18, "unit": 18, "dropout": 18, "rate": 18, "normal": [18, 22], "polyak_upd": 18, "target_param": 18, "polyak": 18, "coeffici": 18, "usual": 18, "small": 18, "paretoarch": 19, "convex_hul": 19, "archiv": 19, "ineffici": 19, "filter_convex_domin": 19, "fast": 19, "hull": 19, "leverag": 19, "quickhul": 19, "first": 19, "filter_pareto_domin": 19, "remove_dupl": 19, "duplic": 19, "get_non_domin": 19, "subset": 19, "stackoverflow": 19, "32791911": 19, "answer": 19, "wrong": 19, "import": 19, "made": [19, 20], "get_non_dominated_ind": 19, "solut": 19, "boolean": 19, "get_non_pareto_dominated_ind": 19, "kept": 19, "form": 19, "mostli": 20, "pymoo": [20, 22], "axiomat": 20, "hv": 20, "customli": 20, "expected_util": 20, "weights_set": 20, "similar": 20, "But": 20, "need": 20, "assess": 20, "product": [20, 21], "_supportsarrai": 20, "dtype": 20, "_nestedsequ": 20, "complex": 20, "byte": 20, "known_front": 20, "current_estim": 20, "nearest": 20, "maximum_utility_loss": 20, "reference_set": 20, "basic": 20, "tchebicheff": 21, "seen": 21, "compon": 21, "sure": 21, "equally_spaced_weight": 22, "dim": 22, "riesz": 22, "energi": 22, "misc": [22, 23], "reference_direct": 22, "extrema_weight": 22, "extrema": 22, "rest": 22, "random_weight": 22, "dist": 22, "dirichlet": 22, "gaussian": 22, "distribut": 22, "equival": 22, "uniformli": 22, "aim": 23, "reliabl": 23, "strictli": 23, "standard": 23, "mdp": 23, "momdp": 23, "suggest": 23, "read": 23, "under": 23, "criteria": 23, "report": 23, "bias": 23, "dashboard": 23, "lint": 23, "enforc": 23, "pre": 23, "commit": 23, "hook": 23, "well": 23, "etc": [23, 24], "manner": 23, "hyper": 23, "particip": 23, "popular": 23, "stabl": 23, "ai": 23, "43": 23, "experiment": 23, "protocol": 23, "websit": 23, "exampl": [23, 24], "titl": 23, "year": 23, "publish": 23, "howpublish": 23, "url": 23, "As": 24, "much": 24, "repo": 24, "tri": 24, "rule": 24, "structur": 24, "recur": 24, "concept": 24, "neural": 24}, "objects": {"morl_baselines.common.accrued_reward_buffer": [[15, 0, 1, "", "AccruedRewardReplayBuffer"]], "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer": [[15, 1, 1, "", "add"], [15, 1, 1, "", "cleanup"], [15, 1, 1, "", "get_all_data"], [15, 1, 1, "", "sample"]], "morl_baselines.common.buffer": [[15, 0, 1, "", "ReplayBuffer"]], "morl_baselines.common.buffer.ReplayBuffer": [[15, 1, 1, "", "add"], [15, 1, 1, "", "get_all_data"], [15, 1, 1, "", "sample"], [15, 1, 1, "", "sample_obs"]], "morl_baselines.common.diverse_buffer": [[15, 0, 1, "", "DiverseMemory"]], "morl_baselines.common.diverse_buffer.DiverseMemory": [[15, 1, 1, "", "add"], [15, 1, 1, "", "add_sample"], [15, 1, 1, "", "add_tree"], [15, 1, 1, "", "dupe"], [15, 1, 1, "", "extract_trace"], [15, 1, 1, "", "get"], [15, 1, 1, "", "get_data"], [15, 1, 1, "", "get_error"], [15, 1, 1, "", "get_sec_write"], [15, 1, 1, "", "get_trace_value"], [15, 1, 1, "", "main_mem_is_full"], [15, 1, 1, "", "move_to_sec"], [15, 1, 1, "", "remove_trace"], [15, 1, 1, "", "sample"], [15, 1, 1, "", "sec_distances"], [15, 1, 1, "", "update"]], "morl_baselines.common": [[16, 2, 0, "-", "evaluation"], [18, 2, 0, "-", "networks"], [19, 2, 0, "-", "pareto"], [20, 2, 0, "-", "performance_indicators"], [21, 2, 0, "-", "scalarization"], [17, 2, 0, "-", "utils"], [22, 2, 0, "-", "weights"]], "morl_baselines.common.evaluation": [[16, 3, 1, "", "eval_mo"], [16, 3, 1, "", "eval_mo_reward_conditioned"], [16, 3, 1, "", "log_all_multi_policy_metrics"], [16, 3, 1, "", "log_episode_info"], [16, 3, 1, "", "policy_evaluation_mo"], [16, 3, 1, "", "seed_everything"]], "morl_baselines.common.networks": [[18, 0, 1, "", "NatureCNN"], [18, 3, 1, "", "get_grad_norm"], [18, 3, 1, "", "huber"], [18, 3, 1, "", "layer_init"], [18, 3, 1, "", "mlp"], [18, 3, 1, "", "polyak_update"]], "morl_baselines.common.networks.NatureCNN": [[18, 1, 1, "", "forward"]], "morl_baselines.common.pareto": [[19, 0, 1, "", "ParetoArchive"], [19, 3, 1, "", "filter_convex_dominated"], [19, 3, 1, "", "filter_pareto_dominated"], [19, 3, 1, "", "get_non_dominated"], [19, 3, 1, "", "get_non_dominated_inds"], [19, 3, 1, "", "get_non_pareto_dominated_inds"]], "morl_baselines.common.pareto.ParetoArchive": [[19, 1, 1, "", "add"]], "morl_baselines.common.performance_indicators": [[20, 3, 1, "", "expected_utility"], [20, 3, 1, "", "hypervolume"], [20, 3, 1, "", "igd"], [20, 3, 1, "", "maximum_utility_loss"], [20, 3, 1, "", "sparsity"]], "morl_baselines.common.prioritized_buffer": [[15, 0, 1, "", "PrioritizedReplayBuffer"]], "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer": [[15, 1, 1, "", "add"], [15, 1, 1, "", "get_all_data"], [15, 1, 1, "", "sample"], [15, 1, 1, "", "sample_obs"], [15, 1, 1, "", "update_priorities"]], "morl_baselines.common.scalarization": [[21, 3, 1, "", "tchebicheff"], [21, 3, 1, "", "weighted_sum"]], "morl_baselines.common.utils": [[17, 3, 1, "", "linearly_decaying_value"], [17, 3, 1, "", "make_gif"], [17, 3, 1, "", "unique_tol"]], "morl_baselines.common.weights": [[22, 3, 1, "", "equally_spaced_weights"], [22, 3, 1, "", "extrema_weights"], [22, 3, 1, "", "random_weights"]], "morl_baselines.multi_policy.capql.capql": [[2, 0, 1, "", "CAPQL"]], "morl_baselines.multi_policy.capql.capql.CAPQL": [[2, 1, 1, "", "eval"], [2, 1, 1, "", "get_config"], [2, 1, 1, "", "load"], [2, 1, 1, "", "save"], [2, 1, 1, "", "train"], [2, 1, 1, "", "update"]], "morl_baselines.multi_policy.envelope.envelope": [[3, 0, 1, "", "Envelope"]], "morl_baselines.multi_policy.envelope.envelope.Envelope": [[3, 1, 1, "", "act"], [3, 1, 1, "", "ddqn_target"], [3, 1, 1, "", "envelope_target"], [3, 1, 1, "", "eval"], [3, 1, 1, "", "get_config"], [3, 1, 1, "", "load"], [3, 1, 1, "", "max_action"], [3, 1, 1, "", "save"], [3, 1, 1, "", "train"], [3, 1, 1, "", "update"]], "morl_baselines.multi_policy.gpi_pd.gpi_pd": [[4, 0, 1, "", "GPIPD"]], "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD": [[4, 1, 1, "", "eval"], [4, 1, 1, "", "get_config"], [4, 1, 1, "", "gpi_action"], [4, 1, 1, "", "load"], [4, 1, 1, "", "max_action"], [4, 1, 1, "", "save"], [4, 1, 1, "", "set_weight_support"], [4, 1, 1, "", "train"], [4, 1, 1, "", "train_iteration"], [4, 1, 1, "", "update"]], "morl_baselines.multi_policy.linear_support.linear_support": [[5, 0, 1, "", "LinearSupport"]], "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport": [[5, 1, 1, "", "add_solution"], [5, 1, 1, "", "compute_corner_weights"], [5, 1, 1, "", "ended"], [5, 1, 1, "", "get_corner_weights"], [5, 1, 1, "", "get_weight_support"], [5, 1, 1, "", "gpi_ls_priority"], [5, 1, 1, "", "is_dominated"], [5, 1, 1, "", "max_scalarized_value"], [5, 1, 1, "", "max_value_lp"], [5, 1, 1, "", "next_weight"], [5, 1, 1, "", "ols_priority"], [5, 1, 1, "", "remove_obsolete_values"], [5, 1, 1, "", "remove_obsolete_weights"]], "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning": [[6, 0, 1, "", "MPMOQLearning"]], "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning": [[6, 1, 1, "", "delete_policies"], [6, 1, 1, "", "eval"], [6, 1, 1, "", "get_config"], [6, 1, 1, "", "max_scalar_q_value"], [6, 1, 1, "", "train"]], "morl_baselines.multi_policy.pareto_q_learning.pql": [[7, 0, 1, "", "PQL"]], "morl_baselines.multi_policy.pareto_q_learning.pql.PQL": [[7, 1, 1, "", "calc_non_dominated"], [7, 1, 1, "", "get_config"], [7, 1, 1, "", "get_local_pcs"], [7, 1, 1, "", "get_q_set"], [7, 1, 1, "", "score_hypervolume"], [7, 1, 1, "", "score_pareto_cardinality"], [7, 1, 1, "", "select_action"], [7, 1, 1, "", "track_policy"], [7, 1, 1, "", "train"]], "morl_baselines.multi_policy.pcn.pcn": [[8, 0, 1, "", "PCN"]], "morl_baselines.multi_policy.pcn.pcn.PCN": [[8, 1, 1, "", "eval"], [8, 1, 1, "", "evaluate"], [8, 1, 1, "", "get_config"], [8, 1, 1, "", "save"], [8, 1, 1, "", "set_desired_return_and_horizon"], [8, 1, 1, "", "train"], [8, 1, 1, "", "update"]], "morl_baselines.multi_policy.pgmorl.pgmorl": [[9, 0, 1, "", "PGMORL"], [9, 0, 1, "", "PerformancePredictor"]], "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL": [[9, 1, 1, "", "get_config"], [9, 1, 1, "", "train"]], "morl_baselines.multi_policy.pgmorl.pgmorl.PerformancePredictor": [[9, 1, 1, "", "add"], [9, 1, 1, "", "predict_next_evaluation"]], "morl_baselines.single_policy.esr.eupg": [[12, 0, 1, "", "EUPG"]], "morl_baselines.single_policy.esr.eupg.EUPG": [[12, 1, 1, "", "eval"], [12, 1, 1, "", "get_config"], [12, 1, 1, "", "train"], [12, 1, 1, "", "update"]], "morl_baselines.single_policy.ser.mo_ppo": [[9, 0, 1, "", "MOPPO"]], "morl_baselines.single_policy.ser.mo_ppo.MOPPO": [[9, 1, 1, "", "change_weights"], [9, 1, 1, "", "eval"], [9, 1, 1, "", "train"], [9, 1, 1, "", "update"]], "morl_baselines.single_policy.ser.mo_q_learning": [[13, 0, 1, "", "MOQLearning"]], "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning": [[13, 1, 1, "", "eval"], [13, 1, 1, "", "get_config"], [13, 1, 1, "", "scalarized_q_values"], [13, 1, 1, "", "train"], [13, 1, 1, "", "update"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:module", "3": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "module", "Python module"], "3": ["py", "function", "Python function"]}, "titleterms": {"overview": [0, 24], "multi": [1, 10, 15, 23], "polici": [1, 10, 11], "algorithm": [1, 10, 11, 23], "concav": 2, "augment": 2, "pareto": [2, 7, 8, 19], "q": [2, 3, 7], "learn": [2, 3, 6, 7, 13, 23], "capql": 2, "envelop": 3, "gpi": 4, "priorit": [4, 15], "dyna": 4, "linear": 5, "support": 5, "mpmoq": 6, "condit": 8, "network": [8, 18], "pgmorl": 9, "applic": 9, "limit": 9, "principl": 9, "moppo": 9, "weight": [9, 22], "gener": 9, "predict": 9, "model": 9, "perform": [10, 20], "assess": 10, "introduct": 10, "metric": 10, "singl": [10, 11], "storag": 10, "benchmark": [10, 23], "script": 10, "refer": 10, "eupg": 12, "moq": 13, "commun": 14, "maintain": 14, "contribut": 14, "acknowledg": 14, "replai": 15, "buffer": 15, "object": [15, 23], "divers": 15, "accru": 15, "reward": 15, "evalu": 16, "miscellan": 17, "neural": 18, "helper": [18, 22], "util": 19, "indic": 20, "scalar": 21, "function": 21, "morl": 23, "baselin": 23, "A": 23, "collect": 23, "reinforc": 23, "featur": 23, "cite": 23}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"Overview": [[0, "overview"], [24, "overview"]], "Multi-Policy Algorithms": [[1, "multi-policy-algorithms"]], "Concave-Augmented Pareto Q-Learning (CAPQL)": [[2, "concave-augmented-pareto-q-learning-capql"]], "Envelope Q-Learning": [[3, "envelope-q-learning"]], "GPI-Prioritized Dyna": [[4, "gpi-prioritized-dyna"]], "Linear Support": [[5, "linear-support"]], "MPMOQ Learning": [[6, "mpmoq-learning"]], "Pareto Q-Learning": [[7, "pareto-q-learning"]], "Pareto Conditioned Networks": [[8, "pareto-conditioned-networks"]], "PGMORL": [[9, "pgmorl"], [9, "id1"]], "Applicability and limitations": [[9, "applicability-and-limitations"]], "Principle": [[9, "principle"]], "MOPPO": [[9, "moppo"]], "Weight generator - prediction model": [[9, "weight-generator-prediction-model"]], "Performance assessments": [[10, "performance-assessments"]], "Introduction": [[10, "introduction"]], "Metrics": [[10, "metrics"]], "Single-policy algorithms": [[10, "single-policy-algorithms"]], "Multi-policy algorithms": [[10, "multi-policy-algorithms"]], "Storage": [[10, "storage"]], "Benchmarking script": [[10, "benchmarking-script"]], "Algorithms": [[10, "algorithms"]], "References": [[10, "references"]], "Single-policy Algorithms": [[11, "single-policy-algorithms"]], "EUPG": [[12, "eupg"]], "MOQ-Learning": [[13, "moq-learning"]], "Community": [[14, "community"]], "Maintainers": [[14, "maintainers"]], "Contributing": [[14, "contributing"]], "Acknowledgements": [[14, "acknowledgements"]], "Replay Buffers": [[15, "replay-buffers"]], "Multi-Objective Replay Buffer": [[15, "multi-objective-replay-buffer"]], "Diverse Replay Buffer": [[15, "diverse-replay-buffer"]], "Prioritized Replay Buffer": [[15, "prioritized-replay-buffer"]], "Accrued Reward Replay Buffer": [[15, "accrued-reward-replay-buffer"]], "Evaluations": [[16, "module-morl_baselines.common.evaluation"]], "Miscellaneous": [[17, "module-morl_baselines.common.utils"]], "Neural Networks helpers": [[18, "module-morl_baselines.common.networks"]], "Pareto utils": [[19, "module-morl_baselines.common.pareto"]], "Performance indicators": [[20, "module-morl_baselines.common.performance_indicators"]], "Scalarization functions": [[21, "module-morl_baselines.common.scalarization"]], "Weights helpers": [[22, "module-morl_baselines.common.weights"]], "MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.": [[23, "morl-baselines-a-collection-of-multi-objective-reinforcement-learning-algorithms"]], "Features of MORL-Baselines": [[23, "features-of-morl-baselines"]], "Benchmarks": [[23, "benchmarks"]], "Citing MORL-Baselines": [[23, "citing-morl-baselines"]]}, "indexentries": {"capql (class in morl_baselines.multi_policy.capql.capql)": [[2, "morl_baselines.multi_policy.capql.capql.CAPQL"]], "eval() (morl_baselines.multi_policy.capql.capql.capql method)": [[2, "morl_baselines.multi_policy.capql.capql.CAPQL.eval"]], "get_config() (morl_baselines.multi_policy.capql.capql.capql method)": [[2, "morl_baselines.multi_policy.capql.capql.CAPQL.get_config"]], "load() (morl_baselines.multi_policy.capql.capql.capql method)": [[2, "morl_baselines.multi_policy.capql.capql.CAPQL.load"]], "save() (morl_baselines.multi_policy.capql.capql.capql method)": [[2, "morl_baselines.multi_policy.capql.capql.CAPQL.save"]], "train() (morl_baselines.multi_policy.capql.capql.capql method)": [[2, "morl_baselines.multi_policy.capql.capql.CAPQL.train"]], "update() (morl_baselines.multi_policy.capql.capql.capql method)": [[2, "morl_baselines.multi_policy.capql.capql.CAPQL.update"]], "envelope (class in morl_baselines.multi_policy.envelope.envelope)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope"]], "act() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.act"]], "ddqn_target() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.ddqn_target"]], "envelope_target() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.envelope_target"]], "eval() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.eval"]], "get_config() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.get_config"]], "load() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.load"]], "max_action() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.max_action"]], "save() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.save"]], "train() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.train"]], "update() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[3, "morl_baselines.multi_policy.envelope.envelope.Envelope.update"]], "gpipd (class in morl_baselines.multi_policy.gpi_pd.gpi_pd)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD"]], "eval() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.eval"]], "get_config() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.get_config"]], "gpi_action() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.gpi_action"]], "load() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.load"]], "max_action() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.max_action"]], "save() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.save"]], "set_weight_support() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.set_weight_support"]], "train() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.train"]], "train_iteration() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.train_iteration"]], "update() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[4, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.update"]], "linearsupport (class in morl_baselines.multi_policy.linear_support.linear_support)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport"]], "add_solution() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.add_solution"]], "compute_corner_weights() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.compute_corner_weights"]], "ended() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.ended"]], "get_corner_weights() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.get_corner_weights"]], "get_weight_support() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.get_weight_support"]], "gpi_ls_priority() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.gpi_ls_priority"]], "is_dominated() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.is_dominated"]], "max_scalarized_value() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.max_scalarized_value"]], "max_value_lp() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.max_value_lp"]], "next_weight() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.next_weight"]], "ols_priority() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.ols_priority"]], "remove_obsolete_values() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.remove_obsolete_values"]], "remove_obsolete_weights() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[5, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.remove_obsolete_weights"]], "mpmoqlearning (class in morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning)": [[6, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning"]], "delete_policies() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[6, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.delete_policies"]], "eval() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[6, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.eval"]], "get_config() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[6, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.get_config"]], "max_scalar_q_value() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[6, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.max_scalar_q_value"]], "train() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[6, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.train"]], "pql (class in morl_baselines.multi_policy.pareto_q_learning.pql)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL"]], "calc_non_dominated() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.calc_non_dominated"]], "get_config() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_config"]], "get_local_pcs() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_local_pcs"]], "get_q_set() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_q_set"]], "score_hypervolume() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.score_hypervolume"]], "score_pareto_cardinality() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.score_pareto_cardinality"]], "select_action() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.select_action"]], "track_policy() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.track_policy"]], "train() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[7, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.train"]], "pcn (class in morl_baselines.multi_policy.pcn.pcn)": [[8, "morl_baselines.multi_policy.pcn.pcn.PCN"]], "eval() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[8, "morl_baselines.multi_policy.pcn.pcn.PCN.eval"]], "evaluate() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[8, "morl_baselines.multi_policy.pcn.pcn.PCN.evaluate"]], "get_config() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[8, "morl_baselines.multi_policy.pcn.pcn.PCN.get_config"]], "save() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[8, "morl_baselines.multi_policy.pcn.pcn.PCN.save"]], "set_desired_return_and_horizon() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[8, "morl_baselines.multi_policy.pcn.pcn.PCN.set_desired_return_and_horizon"]], "train() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[8, "morl_baselines.multi_policy.pcn.pcn.PCN.train"]], "update() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[8, "morl_baselines.multi_policy.pcn.pcn.PCN.update"]], "moppo (class in morl_baselines.single_policy.ser.mo_ppo)": [[9, "morl_baselines.single_policy.ser.mo_ppo.MOPPO"]], "pgmorl (class in morl_baselines.multi_policy.pgmorl.pgmorl)": [[9, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL"]], "performancepredictor (class in morl_baselines.multi_policy.pgmorl.pgmorl)": [[9, "morl_baselines.multi_policy.pgmorl.pgmorl.PerformancePredictor"]], "add() (morl_baselines.multi_policy.pgmorl.pgmorl.performancepredictor method)": [[9, "morl_baselines.multi_policy.pgmorl.pgmorl.PerformancePredictor.add"]], "change_weights() (morl_baselines.single_policy.ser.mo_ppo.moppo method)": [[9, "morl_baselines.single_policy.ser.mo_ppo.MOPPO.change_weights"]], "eval() (morl_baselines.single_policy.ser.mo_ppo.moppo method)": [[9, "morl_baselines.single_policy.ser.mo_ppo.MOPPO.eval"]], "get_config() (morl_baselines.multi_policy.pgmorl.pgmorl.pgmorl method)": [[9, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL.get_config"]], "predict_next_evaluation() (morl_baselines.multi_policy.pgmorl.pgmorl.performancepredictor method)": [[9, "morl_baselines.multi_policy.pgmorl.pgmorl.PerformancePredictor.predict_next_evaluation"]], "train() (morl_baselines.multi_policy.pgmorl.pgmorl.pgmorl method)": [[9, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL.train"]], "train() (morl_baselines.single_policy.ser.mo_ppo.moppo method)": [[9, "morl_baselines.single_policy.ser.mo_ppo.MOPPO.train"]], "update() (morl_baselines.single_policy.ser.mo_ppo.moppo method)": [[9, "morl_baselines.single_policy.ser.mo_ppo.MOPPO.update"]], "eupg (class in morl_baselines.single_policy.esr.eupg)": [[12, "morl_baselines.single_policy.esr.eupg.EUPG"]], "eval() (morl_baselines.single_policy.esr.eupg.eupg method)": [[12, "morl_baselines.single_policy.esr.eupg.EUPG.eval"]], "get_config() (morl_baselines.single_policy.esr.eupg.eupg method)": [[12, "morl_baselines.single_policy.esr.eupg.EUPG.get_config"]], "train() (morl_baselines.single_policy.esr.eupg.eupg method)": [[12, "morl_baselines.single_policy.esr.eupg.EUPG.train"]], "update() (morl_baselines.single_policy.esr.eupg.eupg method)": [[12, "morl_baselines.single_policy.esr.eupg.EUPG.update"]], "moqlearning (class in morl_baselines.single_policy.ser.mo_q_learning)": [[13, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning"]], "eval() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[13, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.eval"]], "get_config() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[13, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.get_config"]], "scalarized_q_values() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[13, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.scalarized_q_values"]], "train() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[13, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.train"]], "update() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[13, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.update"]], "accruedrewardreplaybuffer (class in morl_baselines.common.accrued_reward_buffer)": [[15, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer"]], "diversememory (class in morl_baselines.common.diverse_buffer)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory"]], "prioritizedreplaybuffer (class in morl_baselines.common.prioritized_buffer)": [[15, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer"]], "replaybuffer (class in morl_baselines.common.buffer)": [[15, "morl_baselines.common.buffer.ReplayBuffer"]], "add() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[15, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.add"]], "add() (morl_baselines.common.buffer.replaybuffer method)": [[15, "morl_baselines.common.buffer.ReplayBuffer.add"]], "add() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.add"]], "add() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[15, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.add"]], "add_sample() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.add_sample"]], "add_tree() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.add_tree"]], "cleanup() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[15, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.cleanup"]], "dupe() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.dupe"]], "extract_trace() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.extract_trace"]], "get() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.get"]], "get_all_data() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[15, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.get_all_data"]], "get_all_data() (morl_baselines.common.buffer.replaybuffer method)": [[15, "morl_baselines.common.buffer.ReplayBuffer.get_all_data"]], "get_all_data() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[15, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.get_all_data"]], "get_data() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.get_data"]], "get_error() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.get_error"]], "get_sec_write() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.get_sec_write"]], "get_trace_value() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.get_trace_value"]], "main_mem_is_full() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.main_mem_is_full"]], "move_to_sec() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.move_to_sec"]], "remove_trace() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.remove_trace"]], "sample() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[15, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.sample"]], "sample() (morl_baselines.common.buffer.replaybuffer method)": [[15, "morl_baselines.common.buffer.ReplayBuffer.sample"]], "sample() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.sample"]], "sample() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[15, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.sample"]], "sample_obs() (morl_baselines.common.buffer.replaybuffer method)": [[15, "morl_baselines.common.buffer.ReplayBuffer.sample_obs"]], "sample_obs() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[15, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.sample_obs"]], "sec_distances() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.sec_distances"]], "update() (morl_baselines.common.diverse_buffer.diversememory method)": [[15, "morl_baselines.common.diverse_buffer.DiverseMemory.update"]], "update_priorities() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[15, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.update_priorities"]], "eval_mo() (in module morl_baselines.common.evaluation)": [[16, "morl_baselines.common.evaluation.eval_mo"]], "eval_mo_reward_conditioned() (in module morl_baselines.common.evaluation)": [[16, "morl_baselines.common.evaluation.eval_mo_reward_conditioned"]], "log_all_multi_policy_metrics() (in module morl_baselines.common.evaluation)": [[16, "morl_baselines.common.evaluation.log_all_multi_policy_metrics"]], "log_episode_info() (in module morl_baselines.common.evaluation)": [[16, "morl_baselines.common.evaluation.log_episode_info"]], "module": [[16, "module-morl_baselines.common.evaluation"], [17, "module-morl_baselines.common.utils"], [18, "module-morl_baselines.common.networks"], [19, "module-morl_baselines.common.pareto"], [20, "module-morl_baselines.common.performance_indicators"], [21, "module-morl_baselines.common.scalarization"], [22, "module-morl_baselines.common.weights"]], "morl_baselines.common.evaluation": [[16, "module-morl_baselines.common.evaluation"]], "policy_evaluation_mo() (in module morl_baselines.common.evaluation)": [[16, "morl_baselines.common.evaluation.policy_evaluation_mo"]], "seed_everything() (in module morl_baselines.common.evaluation)": [[16, "morl_baselines.common.evaluation.seed_everything"]], "linearly_decaying_value() (in module morl_baselines.common.utils)": [[17, "morl_baselines.common.utils.linearly_decaying_value"]], "make_gif() (in module morl_baselines.common.utils)": [[17, "morl_baselines.common.utils.make_gif"]], "morl_baselines.common.utils": [[17, "module-morl_baselines.common.utils"]], "unique_tol() (in module morl_baselines.common.utils)": [[17, "morl_baselines.common.utils.unique_tol"]], "naturecnn (class in morl_baselines.common.networks)": [[18, "morl_baselines.common.networks.NatureCNN"]], "forward() (morl_baselines.common.networks.naturecnn method)": [[18, "morl_baselines.common.networks.NatureCNN.forward"]], "get_grad_norm() (in module morl_baselines.common.networks)": [[18, "morl_baselines.common.networks.get_grad_norm"]], "huber() (in module morl_baselines.common.networks)": [[18, "morl_baselines.common.networks.huber"]], "layer_init() (in module morl_baselines.common.networks)": [[18, "morl_baselines.common.networks.layer_init"]], "mlp() (in module morl_baselines.common.networks)": [[18, "morl_baselines.common.networks.mlp"]], "morl_baselines.common.networks": [[18, "module-morl_baselines.common.networks"]], "polyak_update() (in module morl_baselines.common.networks)": [[18, "morl_baselines.common.networks.polyak_update"]], "paretoarchive (class in morl_baselines.common.pareto)": [[19, "morl_baselines.common.pareto.ParetoArchive"]], "add() (morl_baselines.common.pareto.paretoarchive method)": [[19, "morl_baselines.common.pareto.ParetoArchive.add"]], "filter_convex_dominated() (in module morl_baselines.common.pareto)": [[19, "morl_baselines.common.pareto.filter_convex_dominated"]], "filter_pareto_dominated() (in module morl_baselines.common.pareto)": [[19, "morl_baselines.common.pareto.filter_pareto_dominated"]], "get_non_dominated() (in module morl_baselines.common.pareto)": [[19, "morl_baselines.common.pareto.get_non_dominated"]], "get_non_dominated_inds() (in module morl_baselines.common.pareto)": [[19, "morl_baselines.common.pareto.get_non_dominated_inds"]], "get_non_pareto_dominated_inds() (in module morl_baselines.common.pareto)": [[19, "morl_baselines.common.pareto.get_non_pareto_dominated_inds"]], "morl_baselines.common.pareto": [[19, "module-morl_baselines.common.pareto"]], "expected_utility() (in module morl_baselines.common.performance_indicators)": [[20, "morl_baselines.common.performance_indicators.expected_utility"]], "hypervolume() (in module morl_baselines.common.performance_indicators)": [[20, "morl_baselines.common.performance_indicators.hypervolume"]], "igd() (in module morl_baselines.common.performance_indicators)": [[20, "morl_baselines.common.performance_indicators.igd"]], "maximum_utility_loss() (in module morl_baselines.common.performance_indicators)": [[20, "morl_baselines.common.performance_indicators.maximum_utility_loss"]], "morl_baselines.common.performance_indicators": [[20, "module-morl_baselines.common.performance_indicators"]], "sparsity() (in module morl_baselines.common.performance_indicators)": [[20, "morl_baselines.common.performance_indicators.sparsity"]], "morl_baselines.common.scalarization": [[21, "module-morl_baselines.common.scalarization"]], "tchebicheff() (in module morl_baselines.common.scalarization)": [[21, "morl_baselines.common.scalarization.tchebicheff"]], "weighted_sum() (in module morl_baselines.common.scalarization)": [[21, "morl_baselines.common.scalarization.weighted_sum"]], "equally_spaced_weights() (in module morl_baselines.common.weights)": [[22, "morl_baselines.common.weights.equally_spaced_weights"]], "extrema_weights() (in module morl_baselines.common.weights)": [[22, "morl_baselines.common.weights.extrema_weights"]], "morl_baselines.common.weights": [[22, "module-morl_baselines.common.weights"]], "random_weights() (in module morl_baselines.common.weights)": [[22, "morl_baselines.common.weights.random_weights"]]}})