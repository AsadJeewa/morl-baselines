Search.setIndex({"docnames": ["algos/algorithms", "algos/multi_policy", "algos/multi_policy/envelope", "algos/multi_policy/mp_mo_q_learning", "algos/multi_policy/ols", "algos/multi_policy/pareto_q_learning", "algos/multi_policy/pgmorl", "algos/performances", "algos/single_policy", "algos/single_policy/eupg", "algos/single_policy/moq_learning", "community/community", "features/buffers", "features/misc", "features/networks", "features/pareto", "features/performance_indicators", "features/scalarization", "index", "quickstart/overview"], "filenames": ["algos/algorithms.md", "algos/multi_policy.md", "algos/multi_policy/envelope.md", "algos/multi_policy/mp_mo_q_learning.md", "algos/multi_policy/ols.md", "algos/multi_policy/pareto_q_learning.md", "algos/multi_policy/pgmorl.md", "algos/performances.md", "algos/single_policy.md", "algos/single_policy/eupg.md", "algos/single_policy/moq_learning.md", "community/community.md", "features/buffers.md", "features/misc.md", "features/networks.md", "features/pareto.md", "features/performance_indicators.md", "features/scalarization.md", "index.md", "quickstart/overview.md"], "titles": ["Overview", "Multi-Policy Algorithms", "Envelope Q-Learning", "MPMOQ Learning", "OLS", "Pareto Q-Learning", "PGMORL", "Performance assessments", "Single-policy Algorithms", "EUPG", "MOQ-Learning", "Community", "Replay Buffers", "Miscellaneous", "Neural Networks helpers", "Pareto utils", "Performance indicators", "Scalarization functions", "MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.", "Overview"], "terms": {"morl": [0, 2, 3, 5, 6, 7, 9, 10, 11, 13, 16, 19], "baselin": [0, 2, 3, 5, 6, 9, 10, 11, 13, 19], "contain": [0, 12, 13, 18, 19], "multipl": [0, 2, 12], "implement": [0, 7, 11, 12, 18, 19], "multi": [0, 2, 3, 5, 6, 9, 10, 14, 16, 19], "object": [0, 2, 3, 4, 5, 6, 7, 9, 10, 16], "reinforc": [0, 2, 3, 5, 6, 7, 9, 10, 12, 14], "learn": [0, 6, 7, 9, 11, 12, 14], "algorithm": [0, 2, 3, 6, 9, 10, 11, 12, 16, 19], "The": [0, 2, 5, 7, 9, 12, 13, 14, 15, 19], "follow": [0, 7, 12, 13, 14, 18, 19], "tabl": [0, 10], "list": [0, 2, 4, 6, 9, 12, 14, 16], "ar": [0, 4, 7, 11, 12, 16, 18], "current": [0, 2, 4, 5, 11, 12, 13, 14, 16], "algo": 0, "singl": [0, 10, 18, 19], "polici": [0, 2, 3, 5, 9, 10, 16, 18, 19], "esr": [0, 9, 12, 18, 19], "ser": [0, 10, 18, 19], "observ": [0, 2, 9, 12, 14], "space": 0, "action": [0, 2, 5, 9, 10, 12], "paper": [0, 2, 3, 4, 5, 6, 7, 9, 10, 14, 16, 18], "envelop": [0, 7], "q": [0, 3, 7, 10, 11], "continu": [0, 6, 7], "discret": [0, 7], "pgmorl": [0, 7, 16], "supplementari": [0, 6], "materi": [0, 6], "pareto": [0, 7, 11, 16, 18], "mo": [0, 3, 6, 7, 10, 18, 19], "mpmoqlearn": [0, 3, 7], "outer": [0, 3, 4], "loop": [0, 3, 4], "moql": 0, "optimist": [0, 4], "linear": [0, 4], "support": [0, 4, 7], "ol": [0, 7], "section": [0, 4], "3": [0, 4, 7], "thesi": [0, 4], "expect": [0, 7, 9, 16], "util": [0, 7, 9, 10, 13, 14, 16, 18], "gradient": [0, 9], "eupg": [0, 7, 11], "warn": [0, 7], "have": [0, 4, 7, 11, 13, 15, 18], "been": [0, 13, 15, 18], "benchmark": 0, "yet": 0, "some": [0, 7, 16], "them": [0, 7, 11], "limit": 0, "featur": [0, 11, 14], "For": [0, 7, 18], "exampl": [0, 19], "i": [0, 2, 4, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19], "2": [0, 6, 7, 12], "class": [2, 3, 4, 5, 6, 9, 10, 12, 14, 15], "morl_baselin": [2, 3, 4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18], "multi_polici": [2, 3, 4, 5, 6, 19], "env": [2, 3, 5, 6, 9, 10], "learning_r": [2, 3, 6, 9, 10], "float": [2, 3, 4, 5, 6, 9, 10, 12, 13, 16, 17], "0": [2, 3, 4, 5, 6, 9, 10, 12, 13], "0003": [2, 6], "initial_epsilon": [2, 3, 5, 10], "01": [2, 12, 13], "final_epsilon": [2, 3, 5, 10], "epsilon_decay_step": [2, 3, 10], "option": [2, 3, 4, 5, 6, 9, 10, 13], "int": [2, 3, 4, 5, 6, 9, 10, 12, 13, 14, 16, 17], "none": [2, 3, 4, 5, 6, 9, 10, 12, 13], "tau": [2, 13, 17], "1": [2, 3, 4, 5, 6, 7, 10, 12, 13], "target_net_update_freq": 2, "1000": [2, 3, 9, 10], "buffer_s": [2, 9], "1000000": 2, "net_arch": [2, 6, 9, 14], "256": 2, "batch_siz": [2, 12], "learning_start": [2, 3, 10], "100": [2, 5, 6], "gradient_upd": 2, "gamma": [2, 3, 5, 6, 9, 10], "99": [2, 5, 9], "max_grad_norm": [2, 6], "bool": [2, 3, 4, 5, 6, 9, 10, 12, 16], "true": [2, 3, 4, 5, 6, 7, 9, 10, 12], "num_sample_w": 2, "4": [2, 6], "per": [2, 5, 10, 14], "per_alpha": 2, "6": [2, 6], "initial_homotopy_lambda": 2, "final_homotopy_lambda": 2, "homotopy_decay_step": 2, "project_nam": [2, 3, 5, 6, 9, 10], "str": [2, 3, 5, 6, 9, 10, 13, 16], "experiment_nam": [2, 3, 5, 6, 9, 10], "log": [2, 3, 5, 6, 9, 10, 13], "devic": [2, 6, 9, 12], "union": [2, 6, 9, 16], "auto": [2, 6, 9], "lean": 2, "us": [2, 5, 7, 9, 10, 12, 13, 14, 16, 19], "condit": [2, 9], "network": [2, 9, 13], "emb": 2, "take": 2, "weight": [2, 4, 7, 9, 10, 13, 16, 17, 18], "input": [2, 13, 14, 15], "main": [2, 11, 12], "chang": [2, 15], "thi": [2, 7, 11, 12, 13, 14, 15, 17, 18, 19], "compar": [2, 7], "scalar": [2, 3, 4, 7, 9, 10, 13], "cn": 2, "dqn": [2, 13, 14], "target": [2, 12, 13], "updat": [2, 9, 10, 12, 13], "r": [2, 16], "yang": 2, "x": [2, 13], "sun": 2, "k": [2, 3, 5, 10], "narasimhan": 2, "A": [2, 3, 5, 7, 9, 10, 12, 13, 16], "gener": [2, 3, 6, 7, 9, 10, 13], "adapt": [2, 17], "arxiv": 2, "1908": 2, "08342": 2, "c": [2, 7], "nov": [2, 6, 7], "2019": 2, "access": 2, "sep": 2, "06": 2, "2021": 2, "onlin": 2, "avail": [2, 6, 7, 12, 18], "http": [2, 4, 6, 7, 12, 15, 18], "org": 2, "ab": 2, "act": 2, "ob": [2, 4, 9, 10, 12], "tensor": [2, 12, 13, 14], "w": [2, 4, 6, 7, 9, 10], "epsilon": [2, 4, 13], "greedili": [2, 10], "select": [2, 4, 5], "an": [2, 4, 5, 7, 11, 14], "given": [2, 5, 9, 12, 13], "paramet": [2, 3, 4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18], "vector": [2, 4, 5, 13, 14, 15, 16, 17], "return": [2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 15, 16, 17, 18], "integ": 2, "repres": [2, 14], "ddqn_target": 2, "doubl": 2, "envelope_target": 2, "sampled_w": 2, "comput": [2, 5, 7, 12, 13, 16], "set": [2, 4, 5, 12, 15, 16, 19], "sampl": [2, 12], "eval": [2, 9, 10], "ndarrai": [2, 3, 4, 5, 6, 9, 10, 13, 14, 15, 16, 17], "give": [2, 9, 12], "best": [2, 9, 10, 17], "np": [2, 4, 9, 16], "arrai": [2, 6, 9, 10, 12, 18], "get_config": [2, 3, 5, 6, 9, 10], "dictionari": [2, 3, 5, 6, 9, 10, 12, 13], "configur": [2, 3, 5, 6, 9, 10], "dict": [2, 3, 5, 6, 9, 10, 13], "config": [2, 3, 6, 9, 10], "load": 2, "path": 2, "load_replay_buff": 2, "model": 2, "replai": [2, 19], "buffer": [2, 9, 18, 19], "specifi": [2, 12], "whether": [2, 10, 12], "too": 2, "max_act": 2, "highest": [2, 4], "valu": [2, 4, 5, 7, 12, 13, 16, 17], "save": 2, "save_replay_buff": 2, "save_dir": 2, "filenam": 2, "directori": 2, "train": [2, 3, 5, 6, 9, 10, 13], "total_timestep": [2, 9, 10], "total_episod": 2, "reset_num_timestep": [2, 10], "eval_env": [2, 9, 10], "eval_freq": [2, 3, 9, 10], "reset_learning_start": 2, "fals": [2, 4, 6, 12], "agent": [2, 3, 6, 7, 9, 10, 13], "total": 2, "number": [2, 5, 9, 10, 12, 13, 14], "timestep": [2, 9, 10, 13], "If": [2, 4, 11, 12], "randomli": 2, "everi": [2, 5], "episod": [2, 5, 9, 13], "done": [2, 12], "ignor": 2, "reset": [2, 10], "when": [2, 10, 12], "time": [2, 10], "environ": [2, 7, 9, 10, 18, 19], "evalu": [2, 3, 5, 9, 10, 15], "frequenc": [2, 9], "start": [2, 10, 12], "": [2, 6, 7, 9, 12, 13, 18, 19], "e": [2, 9, 12, 18], "g": [2, 9, 18], "experi": [2, 9, 12, 18], "from": [2, 4, 5, 7, 9, 11, 12, 13, 14, 16, 18], "multi_policy_moqlearn": 3, "mp_mo_q_learn": 3, "ref_point": [3, 5, 6, 16], "numpi": [3, 10, 12, 16, 17, 18], "weights_step_s": 3, "function": [3, 5, 7, 10, 12, 13, 14, 15, 16, 18], "weighted_sum": [3, 10, 17], "9": [3, 10], "type": [3, 10, 14, 16], "num_timestep": 3, "500000": [3, 10], "multipolici": 3, "moq": [3, 7], "version": [3, 7], "mo_q_learn": [3, 10], "van": [3, 5, 10], "moffaert": [3, 5, 10], "m": [3, 7, 10, 16], "drugan": [3, 10], "now": [3, 9, 10], "novel": [3, 10], "design": [3, 10], "techniqu": [3, 10], "2013": [3, 10], "doi": [3, 7, 10], "10": [3, 6, 7, 10], "1109": [3, 10], "adprl": [3, 10], "6615007": [3, 10], "eval_all_ag": 3, "all": [3, 4, 7, 11, 12, 15, 18, 19], "reward": [3, 7, 9, 17, 18], "discount": 3, "tupl": [3, 12], "num_object": 4, "verbos": 4, "method": [4, 5, 7, 10, 12, 13], "next": [4, 12], "roijer": [4, 7, 9, 16], "info": [4, 13], "pub": 4, "pdf": [4, 6], "add_solut": 4, "add": [4, 12, 15], "new": [4, 11, 12], "optim": [4, 5, 7, 18], "indic": [4, 12], "remov": [4, 12, 15], "cc": [4, 7], "being": 4, "domin": [4, 5, 15, 17], "compute_corner_weight": 4, "corner": 4, "see": [4, 19], "definit": [4, 18], "19": 4, "typo": 4, "sign": 4, "should": [4, 12], "end": [4, 12], "queue": 4, "empti": 4, "extrema_weight": 4, "which": [4, 7, 12, 13, 14, 18], "one": [4, 10, 16], "compon": [4, 17], "equal": 4, "rest": 4, "get_ccs_weight": 4, "get_corner_weight": 4, "top_k": 4, "get_prior": 4, "get": [4, 5, 11, 12], "prioriti": [4, 12, 13], "is_domin": 4, "check": [4, 12], "ani": [4, 16], "otherwis": 4, "max_scalarized_valu": 4, "maximum": [4, 7, 12], "max_value_lp": 4, "w_new": 4, "upper": 4, "bound": 4, "next_weight": 4, "remove_obsolete_valu": 4, "visit": 4, "remove_obsolete_weight": 4, "new_valu": 4, "better": 4, "than": 4, "previou": [4, 12], "pareto_q_learn": 5, "pql": 5, "8": 5, "epsilon_decai": 5, "seed": [5, 6, 7, 13], "tabular": 5, "reli": [5, 7, 10, 16, 17], "prune": [5, 18], "now\u00e9": 5, "journal": [5, 18], "machin": [5, 6, 7], "research": 5, "vol": [5, 7], "15": 5, "pp": [5, 6, 7], "3483": 5, "3512": 5, "2014": 5, "calc_non_domin": 5, "state": 5, "non": [5, 15], "get_local_pc": 5, "collect": [5, 14], "local": 5, "pc": 5, "default": [5, 12, 16], "get_q_set": 5, "pair": [5, 12], "score_hypervolum": 5, "score": 5, "base": [5, 7, 9, 16], "upon": 5, "hypervolum": [5, 7, 16], "metric": [5, 16], "score_pareto_cardin": 5, "cardin": 5, "select_act": 5, "score_func": 5, "callabl": [5, 16, 17], "track_polici": 5, "vec": 5, "track": 5, "its": [5, 12], "array_lik": 5, "num_episod": 5, "3000": 5, "log_everi": 5, "action_ev": 5, "front": [5, 7, 15, 16], "result": 5, "name": 5, "final": [5, 13], "env_id": 6, "halfcheetah": 6, "v4": 6, "5": [6, 10], "num_env": 6, "pop_siz": 6, "warmup_iter": 6, "80": 6, "steps_per_iter": 6, "2048": 6, "limit_env_step": 6, "5000000": 6, "evolutionary_iter": 6, "20": 6, "num_weight_candid": 6, "7": 6, "num_performance_buff": 6, "performance_buffer_s": 6, "min_weight": 6, "max_weight": 6, "delta_weight": 6, "995": 6, "torch_determinist": 6, "64": 6, "num_minibatch": 6, "32": 6, "update_epoch": 6, "anneal_lr": 6, "clip_coef": 6, "ent_coef": 6, "vf_coef": 6, "clip_vloss": 6, "norm_adv": 6, "target_kl": 6, "gae": 6, "gae_lambda": 6, "95": 6, "predict": [6, 7, 14], "guid": [6, 7, 18], "refer": [6, 16, 17], "j": [6, 7], "xu": [6, 7], "y": [6, 7], "tian": [6, 7], "p": [6, 7, 16], "ma": [6, 7], "d": [6, 7, 9, 16], "ru": [6, 7], "sueda": [6, 7], "matusik": [6, 7], "robot": [6, 7], "control": [6, 7, 14], "proceed": [6, 7], "37th": [6, 7], "intern": [6, 7], "confer": [6, 7], "2020": [6, 7], "10607": [6, 7], "10616": [6, 7], "mlr": [6, 7], "press": [6, 7], "v119": [6, 7], "xu20h": [6, 7], "html": [6, 7], "peopl": [6, 11], "csail": 6, "mit": 6, "edu": 6, "jiex": 6, "supp": 6, "document": [7, 18, 19], "work": 7, "progress": 7, "To": 7, "ensur": 7, "correct": 7, "we": [7, 11, 12, 16, 18], "want": [7, 11], "test": [7, 18], "variou": [7, 11, 16], "sake": 7, "reproduc": 7, "run": [7, 9], "mainten": 7, "purpos": 7, "long": 7, "term": 7, "conduct": 7, "gymnasium": [7, 18, 19], "henc": 7, "abl": 7, "were": 7, "present": 7, "origin": [7, 11, 18], "propos": 7, "qualiti": [7, 16], "pf": [7, 16], "In": 7, "converg": 7, "divers": 7, "hybrid": 7, "both": [7, 18], "common": [7, 12, 13, 14, 15, 16, 17, 19], "performance_ind": [7, 16], "sparsiti": [7, 16], "averag": [7, 13, 16], "distanc": [7, 12, 16], "between": [7, 10, 16], "each": [7, 10, 12, 14, 16, 17], "consecut": 7, "point": [7, 15, 16, 17], "igd": [7, 16], "sota": 7, "moo": [7, 16], "literatur": 7, "It": [7, 14, 17, 18], "requir": [7, 17], "can": [7, 11], "posteriori": 7, "That": 7, "do": 7, "merg": 7, "found": 7, "respect": 7, "moreov": 7, "assumpt": 7, "user": 7, "These": [7, 12], "allow": 7, "idea": [7, 9, 11], "wherea": 7, "other": [7, 10, 12, 18], "eum": [7, 16], "mul": 7, "loss": [7, 13], "problem": 7, "know": [7, 12], "report": [7, 18], "wandb": [7, 13], "easi": 7, "manipul": 7, "export": 7, "data": [7, 12], "below": [7, 12], "along": 7, "dst": 7, "mountaincar": 7, "fishwood": 7, "fruit": 7, "tree": [7, 12], "mario": 7, "half": 7, "cheetah": 7, "obj": 7, "hopper": 7, "note": 7, "tweak": 7, "posit": [7, 12], "onli": [7, 16, 18], "feel": 7, "modif": 7, "good": 7, "practic": [7, 18], "offici": 7, "instead": 7, "our": [7, 11], "cannot": 7, "gpi": 7, "l": [7, 16], "pd": 7, "minecart": 7, "full": [7, 12], "hay": [7, 11], "et": [7, 13, 14], "al": [7, 13, 14], "plan": [7, 18], "autonom": 7, "system": 7, "36": 7, "apr": 7, "2022": [7, 18], "1007": 7, "s10458": 7, "022": 7, "09552": 7, "zintgraf": [7, 16], "t": [7, 16], "v": [7, 16], "kanter": [7, 16], "f": [7, 11, 16], "oliehoek": [7, 16], "beau": [7, 16], "approach": [7, 16], "2015": [7, 13, 14, 16], "single_polici": [9, 10, 19], "100000": [9, 12], "50": 9, "001": 9, "accru": 9, "futur": 9, "steckelmach": [9, 11], "2018": 9, "accrued_reward": [9, 12], "moqlearn": 10, "id": [10, 12, 13], "parent_writ": 10, "torch": [10, 13, 14], "tensorboard": 10, "writer": [10, 13], "summarywrit": [10, 13], "maintain": 10, "choos": 10, "move": [10, 12], "start_tim": 10, "max": 10, "recal": 10, "launch": 10, "greedi": 10, "discord": 11, "server": 11, "where": 11, "you": 11, "ask": 11, "question": [11, 15], "help": 11, "repositori": [11, 18], "join": 11, "here": 11, "florian": [11, 18], "felten": [11, 18], "ffelten": 11, "luca": [11, 18], "n": [11, 12, 13, 18], "alegr": [11, 18], "lucasalegr": [11, 18], "open": 11, "alwai": [11, 17], "happi": 11, "receiv": 11, "bug": 11, "fix": 11, "discuss": 11, "your": 11, "u": 11, "also": 11, "issu": 11, "pull": 11, "request": 11, "directli": 11, "asid": 11, "contributor": 11, "mani": 11, "who": 11, "project": 11, "wai": [11, 12], "would": 11, "like": 11, "thank": 11, "willem": 11, "r\u00f6pke": 11, "hi": 11, "wilrop": 11, "deni": 11, "conor": 11, "provid": [11, 12, 15, 18], "librari": [12, 18], "replaybuff": 12, "obs_shap": 12, "action_dim": 12, "rew_dim": 12, "max_siz": 12, "obs_dtyp": 12, "float32": 12, "action_dtyp": 12, "next_ob": 12, "get_all_data": 12, "max_sampl": 12, "replac": 12, "use_c": 12, "to_tensor": 12, "batch": 12, "size": [12, 13], "cer": 12, "convert": 12, "pytorch": [12, 18], "sample_ob": 12, "diverse_buff": 12, "diversememori": 12, "main_capac": 12, "sec_capac": 12, "trace_divers": 12, "crowding_divers": 12, "value_funct": 12, "lambda": 12, "integr": 12, "secondari": 12, "code": [12, 15, 18], "extract": 12, "github": [12, 18], "com": [12, 15, 18], "axelabel": 12, "dynmorl": 12, "error": 12, "trace_id": 12, "pred_idx": 12, "tree_id": 12, "proport": 12, "same": 12, "treat": 12, "trace": 12, "determin": 12, "transit": 12, "store": 12, "identifi": 12, "relev": 12, "index": 12, "node": 12, "wa": 12, "add_sampl": 12, "write": 12, "add_tre": 12, "dupe": 12, "trg_i": 12, "src_i": 12, "copi": 12, "sourc": [12, 15], "extract_trac": 12, "those": 12, "get_data": 12, "include_indic": 12, "includ": 12, "get_error": 12, "idx": 12, "correspond": 12, "get_sec_writ": 12, "secondary_trac": 12, "reserved_idx": 12, "find": 12, "free": 12, "spot": 12, "memori": [12, 15], "recurs": 12, "past": 12, "low": 12, "crowd": 12, "get_trace_valu": 12, "trace_tupl": 12, "appli": 12, "main_mem_is_ful": 12, "becaus": 12, "circular": 12, "fill": [12, 13], "suffici": 12, "move_to_sec": 12, "span": 12, "remove_trac": 12, "whose": 12, "sec_dist": 12, "prioritized_buff": 12, "prioritizedreplaybuff": 12, "min_prior": [12, 13], "1e": 12, "05": 12, "update_prior": 12, "accrued_reward_buff": 12, "accruedrewardreplaybuff": 12, "action_shap": 12, "cleanup": 12, "whole": 12, "order": 12, "element": [12, 15], "get_grad_norm": 13, "param": 13, "iter": 13, "how": 13, "grad": 13, "norm": 13, "insid": 13, "nn": [13, 14], "clip_grad_norm_": 13, "huber": 13, "minimum": 13, "layer_init": 13, "layer": [13, 14], "orthogon": 13, "weight_gain": 13, "bias_const": 13, "initi": 13, "gain": 13, "constant": 13, "bia": 13, "linearly_decaying_valu": 13, "initial_valu": 13, "decay_period": 13, "step": 13, "warmup_step": 13, "final_valu": 13, "linearli": 13, "decai": 13, "natur": [13, 14], "schedul": 13, "mnih": [13, 14], "begin": 13, "until": 13, "taken": 13, "period": 13, "over": 13, "complet": 13, "so": [13, 17], "far": [13, 17], "befor": 13, "accord": 13, "log_episode_info": 13, "global_timestep": 13, "inform": 13, "last": 13, "automat": [13, 17, 18], "recordstatisticswrapp": 13, "statist": 13, "global": 13, "polyak_upd": 13, "target_param": 13, "polyak": 13, "coeffici": 13, "usual": 13, "small": 13, "random_weight": 13, "dim": 13, "dist": 13, "gaussian": 13, "random": 13, "normal": 13, "dirichlet": 13, "distribut": 13, "alpha": 13, "either": 13, "naturecnn": 14, "observation_shap": 14, "features_dim": 14, "512": 14, "cnn": 14, "volodymyr": 14, "human": 14, "level": 14, "through": 14, "deep": 14, "518": 14, "7540": 14, "529": 14, "533": 14, "forward": 14, "mlp": 14, "input_dim": 14, "output_dim": 14, "activation_fn": 14, "modul": 14, "activ": 14, "relu": 14, "sequenti": 14, "creat": 14, "perceptron": 14, "fulli": 14, "connect": 14, "dimens": [14, 17], "output": 14, "architectur": 14, "net": [14, 19], "unit": 14, "length": 14, "after": 14, "paretoarch": 15, "archiv": 15, "candid": 15, "ineffici": 15, "get_non_domin": 15, "subset": 15, "stackoverflow": 15, "32791911": 15, "fast": 15, "calcul": 15, "python": 15, "answer": 15, "wrong": 15, "import": 15, "made": [15, 16], "rl": 16, "mostli": 16, "pymoo": 16, "axiomat": 16, "hv": 16, "customli": 16, "expected_util": 16, "weights_set": 16, "dot": [16, 17], "similar": 16, "But": 16, "need": 16, "approxim": 16, "assess": 16, "product": [16, 17], "_supportsarrai": 16, "dtype": 16, "_nestedsequ": 16, "complex": 16, "byte": 16, "basic": 16, "tchebicheff": 17, "reward_dim": 17, "seen": 17, "sure": 17, "sum": 17, "aim": 18, "reliabl": 18, "strictli": 18, "api": 18, "differ": 18, "standard": 18, "detail": [18, 19], "mdp": 18, "momdp": 18, "suggest": 18, "read": 18, "under": 18, "criteria": 18, "perform": 18, "bias": 18, "dashboard": 18, "lint": 18, "format": 18, "enforc": 18, "pre": 18, "commit": 18, "hook": 18, "well": 18, "etc": [18, 19], "against": 18, "ones": 18, "hyper": 18, "misc": 18, "author": 18, "titl": 18, "year": 18, "publish": 18, "howpublish": 18, "url": 18, "As": 19, "much": 19, "possibl": 19, "repo": 19, "tri": 19, "file": 19, "rule": 19, "structur": 19, "recur": 19, "concept": 19, "neural": 19, "more": 19}, "objects": {"morl_baselines.common.accrued_reward_buffer": [[12, 0, 1, "", "AccruedRewardReplayBuffer"]], "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer": [[12, 1, 1, "", "add"], [12, 1, 1, "", "cleanup"], [12, 1, 1, "", "get_all_data"], [12, 1, 1, "", "sample"]], "morl_baselines.common.buffer": [[12, 0, 1, "", "ReplayBuffer"]], "morl_baselines.common.buffer.ReplayBuffer": [[12, 1, 1, "", "add"], [12, 1, 1, "", "get_all_data"], [12, 1, 1, "", "sample"], [12, 1, 1, "", "sample_obs"]], "morl_baselines.common.diverse_buffer": [[12, 0, 1, "", "DiverseMemory"]], "morl_baselines.common.diverse_buffer.DiverseMemory": [[12, 1, 1, "", "add"], [12, 1, 1, "", "add_sample"], [12, 1, 1, "", "add_tree"], [12, 1, 1, "", "dupe"], [12, 1, 1, "", "extract_trace"], [12, 1, 1, "", "get"], [12, 1, 1, "", "get_data"], [12, 1, 1, "", "get_error"], [12, 1, 1, "", "get_sec_write"], [12, 1, 1, "", "get_trace_value"], [12, 1, 1, "", "main_mem_is_full"], [12, 1, 1, "", "move_to_sec"], [12, 1, 1, "", "remove_trace"], [12, 1, 1, "", "sample"], [12, 1, 1, "", "sec_distances"], [12, 1, 1, "", "update"]], "morl_baselines.common": [[14, 2, 0, "-", "networks"], [15, 2, 0, "-", "pareto"], [16, 2, 0, "-", "performance_indicators"], [17, 2, 0, "-", "scalarization"], [13, 2, 0, "-", "utils"]], "morl_baselines.common.networks": [[14, 0, 1, "", "NatureCNN"], [14, 3, 1, "", "mlp"]], "morl_baselines.common.networks.NatureCNN": [[14, 1, 1, "", "forward"]], "morl_baselines.common.pareto": [[15, 0, 1, "", "ParetoArchive"], [15, 3, 1, "", "get_non_dominated"]], "morl_baselines.common.pareto.ParetoArchive": [[15, 1, 1, "", "add"]], "morl_baselines.common.performance_indicators": [[16, 3, 1, "", "expected_utility"], [16, 3, 1, "", "hypervolume"], [16, 3, 1, "", "sparsity"]], "morl_baselines.common.prioritized_buffer": [[12, 0, 1, "", "PrioritizedReplayBuffer"]], "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer": [[12, 1, 1, "", "add"], [12, 1, 1, "", "get_all_data"], [12, 1, 1, "", "sample"], [12, 1, 1, "", "sample_obs"], [12, 1, 1, "", "update_priorities"]], "morl_baselines.common.scalarization": [[17, 3, 1, "", "tchebicheff"], [17, 3, 1, "", "weighted_sum"]], "morl_baselines.common.utils": [[13, 3, 1, "", "get_grad_norm"], [13, 3, 1, "", "huber"], [13, 3, 1, "", "layer_init"], [13, 3, 1, "", "linearly_decaying_value"], [13, 3, 1, "", "log_episode_info"], [13, 3, 1, "", "polyak_update"], [13, 3, 1, "", "random_weights"]], "morl_baselines.multi_policy.envelope.envelope": [[2, 0, 1, "", "Envelope"]], "morl_baselines.multi_policy.envelope.envelope.Envelope": [[2, 1, 1, "", "act"], [2, 1, 1, "", "ddqn_target"], [2, 1, 1, "", "envelope_target"], [2, 1, 1, "", "eval"], [2, 1, 1, "", "get_config"], [2, 1, 1, "", "load"], [2, 1, 1, "", "max_action"], [2, 1, 1, "", "save"], [2, 1, 1, "", "train"], [2, 1, 1, "", "update"]], "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning": [[3, 0, 1, "", "MPMOQLearning"]], "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning": [[3, 1, 1, "", "eval_all_agents"], [3, 1, 1, "", "get_config"], [3, 1, 1, "", "train"]], "morl_baselines.multi_policy.ols.ols": [[4, 0, 1, "", "OLS"]], "morl_baselines.multi_policy.ols.ols.OLS": [[4, 1, 1, "", "add_solution"], [4, 1, 1, "", "compute_corner_weights"], [4, 1, 1, "", "ended"], [4, 1, 1, "", "extrema_weights"], [4, 1, 1, "", "get_ccs_weights"], [4, 1, 1, "", "get_corner_weights"], [4, 1, 1, "", "get_priority"], [4, 1, 1, "", "is_dominated"], [4, 1, 1, "", "max_scalarized_value"], [4, 1, 1, "", "max_value_lp"], [4, 1, 1, "", "next_weight"], [4, 1, 1, "", "remove_obsolete_values"], [4, 1, 1, "", "remove_obsolete_weights"]], "morl_baselines.multi_policy.pareto_q_learning.pql": [[5, 0, 1, "", "PQL"]], "morl_baselines.multi_policy.pareto_q_learning.pql.PQL": [[5, 1, 1, "", "calc_non_dominated"], [5, 1, 1, "", "get_config"], [5, 1, 1, "", "get_local_pcs"], [5, 1, 1, "", "get_q_set"], [5, 1, 1, "", "score_hypervolume"], [5, 1, 1, "", "score_pareto_cardinality"], [5, 1, 1, "", "select_action"], [5, 1, 1, "", "track_policy"], [5, 1, 1, "", "train"]], "morl_baselines.multi_policy.pgmorl.pgmorl": [[6, 0, 1, "", "PGMORL"]], "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL": [[6, 1, 1, "", "get_config"], [6, 1, 1, "", "train"]], "morl_baselines.single_policy.esr.eupg": [[9, 0, 1, "", "EUPG"]], "morl_baselines.single_policy.esr.eupg.EUPG": [[9, 1, 1, "", "eval"], [9, 1, 1, "", "get_config"], [9, 1, 1, "", "train"], [9, 1, 1, "", "update"]], "morl_baselines.single_policy.ser.mo_q_learning": [[10, 0, 1, "", "MOQLearning"]], "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning": [[10, 1, 1, "", "eval"], [10, 1, 1, "", "get_config"], [10, 1, 1, "", "train"], [10, 1, 1, "", "update"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:module", "3": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "module", "Python module"], "3": ["py", "function", "Python function"]}, "titleterms": {"overview": [0, 19], "multi": [1, 7, 12, 18], "polici": [1, 7, 8], "algorithm": [1, 7, 8, 18], "envelop": 2, "q": [2, 5], "learn": [2, 3, 5, 10, 18], "mpmoq": 3, "ol": 4, "pareto": [5, 15], "pgmorl": 6, "perform": [7, 16], "assess": 7, "introduct": 7, "metric": 7, "storag": 7, "singl": [7, 8], "refer": 7, "eupg": 9, "moq": 10, "commun": 11, "maintain": 11, "contribut": 11, "acknowledg": 11, "replai": 12, "buffer": 12, "object": [12, 18], "divers": 12, "priorit": 12, "accru": 12, "reward": 12, "miscellan": 13, "neural": 14, "network": 14, "helper": 14, "util": 15, "indic": 16, "scalar": 17, "function": 17, "morl": 18, "baselin": 18, "A": 18, "collect": 18, "reinforc": 18, "featur": 18, "cite": 18}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"Overview": [[0, "overview"], [19, "overview"]], "Multi-Policy Algorithms": [[1, "multi-policy-algorithms"]], "Envelope Q-Learning": [[2, "envelope-q-learning"]], "MPMOQ Learning": [[3, "mpmoq-learning"]], "OLS": [[4, "ols"]], "Pareto Q-Learning": [[5, "pareto-q-learning"]], "PGMORL": [[6, "pgmorl"]], "Performance assessments": [[7, "performance-assessments"]], "Introduction": [[7, "introduction"]], "Metrics": [[7, "metrics"]], "Storage": [[7, "storage"]], "Algorithms": [[7, "algorithms"]], "Single-policy algorithms": [[7, "single-policy-algorithms"]], "Multi-policy algorithms": [[7, "multi-policy-algorithms"]], "References": [[7, "references"]], "Single-policy Algorithms": [[8, "single-policy-algorithms"]], "EUPG": [[9, "eupg"]], "MOQ-Learning": [[10, "moq-learning"]], "Community": [[11, "community"]], "Maintainers": [[11, "maintainers"]], "Contributing": [[11, "contributing"]], "Acknowledgements": [[11, "acknowledgements"]], "Replay Buffers": [[12, "replay-buffers"]], "Multi-Objective Replay Buffer": [[12, "multi-objective-replay-buffer"]], "Diverse Replay Buffer": [[12, "diverse-replay-buffer"]], "Prioritized Replay Buffer": [[12, "prioritized-replay-buffer"]], "Accrued Reward Replay Buffer": [[12, "accrued-reward-replay-buffer"]], "Miscellaneous": [[13, "module-morl_baselines.common.utils"]], "Neural Networks helpers": [[14, "module-morl_baselines.common.networks"]], "Pareto utils": [[15, "module-morl_baselines.common.pareto"]], "Performance indicators": [[16, "module-morl_baselines.common.performance_indicators"]], "Scalarization functions": [[17, "module-morl_baselines.common.scalarization"]], "MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.": [[18, "morl-baselines-a-collection-of-multi-objective-reinforcement-learning-algorithms"]], "Features of MORL-Baselines": [[18, "features-of-morl-baselines"]], "Citing MORL-Baselines": [[18, "citing-morl-baselines"]]}, "indexentries": {"envelope (class in morl_baselines.multi_policy.envelope.envelope)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope"]], "act() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.act"]], "ddqn_target() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.ddqn_target"]], "envelope_target() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.envelope_target"]], "eval() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.eval"]], "get_config() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.get_config"]], "load() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.load"]], "max_action() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.max_action"]], "save() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.save"]], "train() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.train"]], "update() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.update"]], "mpmoqlearning (class in morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning)": [[3, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning"]], "eval_all_agents() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[3, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.eval_all_agents"]], "get_config() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[3, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.get_config"]], "train() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[3, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.train"]], "ols (class in morl_baselines.multi_policy.ols.ols)": [[4, "morl_baselines.multi_policy.ols.ols.OLS"]], "add_solution() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.add_solution"]], "compute_corner_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.compute_corner_weights"]], "ended() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.ended"]], "extrema_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.extrema_weights"]], "get_ccs_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.get_ccs_weights"]], "get_corner_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.get_corner_weights"]], "get_priority() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.get_priority"]], "is_dominated() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.is_dominated"]], "max_scalarized_value() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.max_scalarized_value"]], "max_value_lp() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.max_value_lp"]], "next_weight() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.next_weight"]], "remove_obsolete_values() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.remove_obsolete_values"]], "remove_obsolete_weights() (morl_baselines.multi_policy.ols.ols.ols method)": [[4, "morl_baselines.multi_policy.ols.ols.OLS.remove_obsolete_weights"]], "pql (class in morl_baselines.multi_policy.pareto_q_learning.pql)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL"]], "calc_non_dominated() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.calc_non_dominated"]], "get_config() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_config"]], "get_local_pcs() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_local_pcs"]], "get_q_set() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_q_set"]], "score_hypervolume() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.score_hypervolume"]], "score_pareto_cardinality() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.score_pareto_cardinality"]], "select_action() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.select_action"]], "track_policy() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.track_policy"]], "train() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[5, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.train"]], "pgmorl (class in morl_baselines.multi_policy.pgmorl.pgmorl)": [[6, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL"]], "get_config() (morl_baselines.multi_policy.pgmorl.pgmorl.pgmorl method)": [[6, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL.get_config"]], "train() (morl_baselines.multi_policy.pgmorl.pgmorl.pgmorl method)": [[6, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL.train"]], "eupg (class in morl_baselines.single_policy.esr.eupg)": [[9, "morl_baselines.single_policy.esr.eupg.EUPG"]], "eval() (morl_baselines.single_policy.esr.eupg.eupg method)": [[9, "morl_baselines.single_policy.esr.eupg.EUPG.eval"]], "get_config() (morl_baselines.single_policy.esr.eupg.eupg method)": [[9, "morl_baselines.single_policy.esr.eupg.EUPG.get_config"]], "train() (morl_baselines.single_policy.esr.eupg.eupg method)": [[9, "morl_baselines.single_policy.esr.eupg.EUPG.train"]], "update() (morl_baselines.single_policy.esr.eupg.eupg method)": [[9, "morl_baselines.single_policy.esr.eupg.EUPG.update"]], "moqlearning (class in morl_baselines.single_policy.ser.mo_q_learning)": [[10, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning"]], "eval() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[10, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.eval"]], "get_config() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[10, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.get_config"]], "train() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[10, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.train"]], "update() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[10, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.update"]], "accruedrewardreplaybuffer (class in morl_baselines.common.accrued_reward_buffer)": [[12, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer"]], "diversememory (class in morl_baselines.common.diverse_buffer)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory"]], "prioritizedreplaybuffer (class in morl_baselines.common.prioritized_buffer)": [[12, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer"]], "replaybuffer (class in morl_baselines.common.buffer)": [[12, "morl_baselines.common.buffer.ReplayBuffer"]], "add() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[12, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.add"]], "add() (morl_baselines.common.buffer.replaybuffer method)": [[12, "morl_baselines.common.buffer.ReplayBuffer.add"]], "add() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.add"]], "add() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[12, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.add"]], "add_sample() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.add_sample"]], "add_tree() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.add_tree"]], "cleanup() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[12, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.cleanup"]], "dupe() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.dupe"]], "extract_trace() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.extract_trace"]], "get() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.get"]], "get_all_data() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[12, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.get_all_data"]], "get_all_data() (morl_baselines.common.buffer.replaybuffer method)": [[12, "morl_baselines.common.buffer.ReplayBuffer.get_all_data"]], "get_all_data() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[12, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.get_all_data"]], "get_data() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.get_data"]], "get_error() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.get_error"]], "get_sec_write() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.get_sec_write"]], "get_trace_value() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.get_trace_value"]], "main_mem_is_full() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.main_mem_is_full"]], "move_to_sec() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.move_to_sec"]], "remove_trace() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.remove_trace"]], "sample() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[12, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.sample"]], "sample() (morl_baselines.common.buffer.replaybuffer method)": [[12, "morl_baselines.common.buffer.ReplayBuffer.sample"]], "sample() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.sample"]], "sample() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[12, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.sample"]], "sample_obs() (morl_baselines.common.buffer.replaybuffer method)": [[12, "morl_baselines.common.buffer.ReplayBuffer.sample_obs"]], "sample_obs() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[12, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.sample_obs"]], "sec_distances() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.sec_distances"]], "update() (morl_baselines.common.diverse_buffer.diversememory method)": [[12, "morl_baselines.common.diverse_buffer.DiverseMemory.update"]], "update_priorities() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[12, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.update_priorities"]], "get_grad_norm() (in module morl_baselines.common.utils)": [[13, "morl_baselines.common.utils.get_grad_norm"]], "huber() (in module morl_baselines.common.utils)": [[13, "morl_baselines.common.utils.huber"]], "layer_init() (in module morl_baselines.common.utils)": [[13, "morl_baselines.common.utils.layer_init"]], "linearly_decaying_value() (in module morl_baselines.common.utils)": [[13, "morl_baselines.common.utils.linearly_decaying_value"]], "log_episode_info() (in module morl_baselines.common.utils)": [[13, "morl_baselines.common.utils.log_episode_info"]], "module": [[13, "module-morl_baselines.common.utils"], [14, "module-morl_baselines.common.networks"], [15, "module-morl_baselines.common.pareto"], [16, "module-morl_baselines.common.performance_indicators"], [17, "module-morl_baselines.common.scalarization"]], "morl_baselines.common.utils": [[13, "module-morl_baselines.common.utils"]], "polyak_update() (in module morl_baselines.common.utils)": [[13, "morl_baselines.common.utils.polyak_update"]], "random_weights() (in module morl_baselines.common.utils)": [[13, "morl_baselines.common.utils.random_weights"]], "naturecnn (class in morl_baselines.common.networks)": [[14, "morl_baselines.common.networks.NatureCNN"]], "forward() (morl_baselines.common.networks.naturecnn method)": [[14, "morl_baselines.common.networks.NatureCNN.forward"]], "mlp() (in module morl_baselines.common.networks)": [[14, "morl_baselines.common.networks.mlp"]], "morl_baselines.common.networks": [[14, "module-morl_baselines.common.networks"]], "paretoarchive (class in morl_baselines.common.pareto)": [[15, "morl_baselines.common.pareto.ParetoArchive"]], "add() (morl_baselines.common.pareto.paretoarchive method)": [[15, "morl_baselines.common.pareto.ParetoArchive.add"]], "get_non_dominated() (in module morl_baselines.common.pareto)": [[15, "morl_baselines.common.pareto.get_non_dominated"]], "morl_baselines.common.pareto": [[15, "module-morl_baselines.common.pareto"]], "expected_utility() (in module morl_baselines.common.performance_indicators)": [[16, "morl_baselines.common.performance_indicators.expected_utility"]], "hypervolume() (in module morl_baselines.common.performance_indicators)": [[16, "morl_baselines.common.performance_indicators.hypervolume"]], "morl_baselines.common.performance_indicators": [[16, "module-morl_baselines.common.performance_indicators"]], "sparsity() (in module morl_baselines.common.performance_indicators)": [[16, "morl_baselines.common.performance_indicators.sparsity"]], "morl_baselines.common.scalarization": [[17, "module-morl_baselines.common.scalarization"]], "tchebicheff() (in module morl_baselines.common.scalarization)": [[17, "morl_baselines.common.scalarization.tchebicheff"]], "weighted_sum() (in module morl_baselines.common.scalarization)": [[17, "morl_baselines.common.scalarization.weighted_sum"]]}})